# ResNet

[论文链接][https://arxiv.org/abs/1512.03385]

Deep Residual Learning for Image Recognition

### idea提出

#### 问题发现

<img src="ResNet.assets/image-20250708142638994.png" alt="image-20250708142638994" style="zoom:67%;" />

论文观察到的问题：层数越多还可能导致训练和测试误差都增大，**层数过多训练不动**，训练误差也随之变大了

由于**dientity mapping**，即**恒等映射**，深的网络应该至少好于浅的网络，因为我可以将深的部分层就不变（类似），这样看，深层应当由于浅层。但是，结果表示没有如此，即没有实现identity mapping

所以该文章显式构造一个identity mapping来解决该问题，即实现深的网络不会比浅的网络更差

#### 问题解决

<img src="ResNet.assets/image-20250708145431976.png" alt="image-20250708145431976" style="zoom: 67%;" />

**下一层去学习上一层预测的值与真实值的差（残差）**

参数、模型复杂度都没变高

### 模型

<img src="ResNet.assets/image-20250708154638671.png" alt="image-20250708154638671" style="zoom: 67%;" />

残差连接如何处理输入输出形状不同的情况

- A：在输入输出上添加额外的0，使得形状对应
- B：采用1*1的卷积层（使其在空间维度不做改变，而在通道维度上改变）这里的通道维度可以手动设置来调整参数复杂度，同时调整步长，使得其在高宽和通道上都能匹配

#### 架构版本

不同版本模型架构

![image-20250708162535907](ResNet.assets/image-20250708162535907.png)

<img src="ResNet.assets/image-20250708172626654.png" alt="image-20250708172626654" style="zoom:67%;" />

层数计算时，不包括 池化层 & softmax层 ，没参数

eg：34层

- 开始一个(7*7,64,stride 2)
- 之后池化(不算层)
- 再两层(3*3,64)作为一个残差块
- ......
- 参照表依次连接
- ......
- 接一个average pool层(不算参数)
- 再接一个全连接层1000-d FC(算一层，有参数（权重 + 偏置），用于线性变换)
  - 1000-d FC：一个具有 1000 维输出的全连接层（Fully Connected Layer）
- 最后一个softmax层(不算层，无参数，用于将输出变为概率分布)

#### 结果分析

<img src="ResNet.assets/image-20250708164037932.png" alt="image-20250708164037932" style="zoom:67%;" />

这里断层是因为设计时，**学习率变化**（这里好像是10次学习率*0.1），所以会有断层跳跃现象

- **这里有不好的地方就是可能导致跳太早，而使收敛效果下降，晚一点跳跃可以使开始方向更准确**

结果

- 有残差的模型收敛更快
- 后期效果更好

#### 三种残差连接方案

**投影操作**（projection shortcut）并**不是发生在残差块内部**主分支的卷积操作中，**而是作用在残差连接的“捷径路径”（shortcut path）上**，**用来对输入做变换**，以便**与主分支的输出维度对齐**。

> **投影（projection）是用于将输入残差（shortcut）映射成与主分支输出具有**相同维度**（包括通道数和空间尺寸），从而可以进行有效的“残差相加”操作。**

举例说明：

- 假设一个残差块中：

  - 主分支的输出是：`[batch_size, 128, 28, 28]`

  - 输入 `x` 是：`[batch_size, 64, 56, 56]`（通道数和尺寸都不同）

  - 此时你不能直接做：

  - ```python
    out = F(x) + x  # ❌ 维度不匹配，不能相加
    ```

- 解决方法就是 **投影 shortcut**：

  - 使用一个 `1×1` 卷积来把 `x` 变成 `[batch_size, 128, 28, 28]`：

  - ```python
    shortcut = nn.Sequential(
        nn.Conv2d(64, 128, kernel_size=1, stride=2, bias=False),
        nn.BatchNorm2d(128)
    )
    ```

  - 这样你就可以合法地做：

  - ```python
    out = F(x) + shortcut(x)  # ✅ 维度一致，可以相加
    ```

(A)

> **对于维度提升，使用零填充的快捷连接（shortcut），且所有快捷连接都不包含参数（与表2和图4右侧相同）。**

- 📘 **解释**：
  - 这是 ResNet（残差网络）中 shortcut（捷径连接）的三种策略之一。
  - 如果前一层的输出维度较小，而后一层需要更大的维度（比如通道数变多），就需要“提升维度”。
  - **零填充**（zero-padding）就是在输入特征图中添加全零通道来对齐尺寸，这种方式不引入新的参数，所以称为“parameter-free”（无参数）。

(B)

> **对于维度提升，使用投影（projection）shortcut，其他情况下的 shortcut 则使用恒等映射（identity）。**

- 📘 **解释**：
  - **维度不一致时**，用一个带参数的 projection（比如 1×1 卷积）将输入映射到新维度；
  - 如果维度一致，直接使用 identity shortcut（恒等映射），即输入直接跳过，不进行任何变换。

(C)

> **所有的 shortcut 都使用投影（projection）方式。**

- 📘 **解释**：
  - 不管维度是否变化，shortcut 全部用 projection（如 1×1 卷积）做线性变换；
  - 这种方式虽然引入更多参数，但理论上对学习能力更强，因为它不依赖输入和输出通道必须一致。

**三种残差连接策略对比**：

| 策略编号 | 维度不一致时的处理方式     | 维度一致时的处理方式 | 是否引入参数 |
| -------- | -------------------------- | -------------------- | ------------ |
| (A)      | **零填充**（zero-padding） | 直接跳过（identity） | ❌ 否         |
| (B)      | **1×1 卷积 projection**    | 直接跳过（identity） | ✅ 是（部分） |
| (C)      | **1×1 卷积 projection**    | 也用 projection      | ✅ 是（全部） |

<img src="ResNet.assets/image-20250708171209302.png" alt="image-20250708171209302" style="zoom: 67%;" />

#### 深层化ResNet方法

在50以上层时，引入bottleneck design（瓶颈设计）

<img src="ResNet.assets/image-20250708172626654.png" alt="image-20250708172626654" style="zoom:67%;" />

深层时

- 使维度增大，学习到更多的模式（上图中 65-d---->256-d ，但是这样会增加4*4的计算复杂度）
- 加入（1*1，64）卷积投影回64-d（降低复杂度）
- 再（1*1，256）卷积投回256-d

**使其算法复杂度差不多**

#### 深层限制

<img src="ResNet.assets/image-20250715140004665.png" alt="image-20250715140004665" style="zoom:50%;" />

加入残差后，在层数达到一定深度后，可能更深的层没有作用了。

#### ResNet训练快的原因探讨

**原因：梯度上保持比较好**

- **不使用残差**

  - 两层
    - <img src="ResNet.assets/image-20250715142123609-1752560489341-1.png" style="zoom: 33%;" />

  - 而梯度都比较小（0的附近），层数多时就过小导致梯度消失

- **使用残差**

  - 两层
    - <img src="ResNet.assets/image-20250715142339550.png" alt="image-20250715142339550" style="zoom: 33%;" />

  - 保持了浅层梯度存在

**梯度不消失 → 梯度能够有效传播 → 网络能更快收敛 → 训练更快、更稳定**

**训练快** ： **收敛得快，而非一轮时间少**