# Transformer

æœ€å¼€å§‹ç”¨äºæœºå™¨ç¿»è¯‘

å¯¹äº**æ—¶åºæ¨¡å‹**

**ä»…ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶**ï¼Œä¸ä½¿ç”¨å·ç§¯å’Œå¾ªç¯

### å‘ç°

**RNN**ï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰ç”±äºæœ‰**æ—¶åº**ï¼ˆéšè—çŠ¶æ€ $h_t=h_{t-1}+t$ï¼‰

- åœ¨**æ—¶é—´ä¸Šæ— æ³•å¹¶è¡Œ**
- æ—¶åºè¾ƒé•¿æ—¶ï¼Œåœ¨åé¢**å…¶å‰é¢çš„ä¿¡æ¯å¯èƒ½æœ‰ä¸¢å¤±**

**CNN**ï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰

- éœ€è¦**ç”¨å¤šå±‚æ¥è¿æ¥**éš”å¾—è¿œçš„åƒç´ 
- ä¼˜ç‚¹ï¼šå¯ä»¥åš**å¤šä¸ªè¾“å‡ºé€šé“**ï¼Œæ¯ä¸ªé€šé“è¯†åˆ«ä¸ä¸€æ ·çš„æ¨¡å¼

**Transformer**

- ä½¿ç”¨**çº¯attention**å®Œå…¨åšå¹¶è¡Œ
- **ä¸€å±‚**å°±å¯ä»¥çœ‹åˆ°**æ•´ä¸ªåºåˆ—**
- ä½¿ç”¨**å¤šå¤´attention**æ¨¡æ‹ŸCNNå¤šè¾“å‡ºé€šé“çš„æ•ˆæœ

Transformerï¼šç¬¬ä¸€ä¸ªä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶åšencodeåˆ°decodeçš„æ¶æ„çš„æ¨¡å‹

## æ¨¡å‹æ¶æ„

**ç¼–ç å™¨ï¼ˆEncoderï¼‰**

- è¾“å…¥ï¼š
  - **æºè¯­è¨€å¥å­çš„åµŒå…¥è¡¨ç¤º**ï¼ˆå¦‚è‹±æ–‡å¥å­ token embeddingï¼‰
- æ¨¡å—å †å ï¼ˆé€šå¸¸ä¸º6å±‚ï¼‰ï¼š
  - å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆ**Self-Attention**ï¼‰
  - å‰é¦ˆå…¨è¿æ¥ç½‘ç»œï¼ˆFeed-Forward Layerï¼‰
  - å±‚å½’ä¸€åŒ–ä¸æ®‹å·®è¿æ¥
- ç›®æ ‡ï¼šæå–å‡ºæ•´å¥ä¸­æ¯ä¸ªå•è¯åœ¨ä¸Šä¸‹æ–‡ä¸­çš„**è¯­ä¹‰è¡¨ç¤º**

**è§£ç å™¨ï¼ˆDecoderï¼‰**

- è¾“å…¥ï¼š
  - **å‰ä¸€æ­¥å·²ç»ç”Ÿæˆçš„è¯**ï¼ˆç›®æ ‡è¯­è¨€çš„å‰ç¼€ï¼‰
  - **ç¼–ç å™¨çš„è¾“å‡º**ï¼ˆæºè¯­è¨€çš„è¯­ä¹‰è¡¨ç¤ºï¼‰
- æ¨¡å—å †å ï¼ˆé€šå¸¸ä¸º6å±‚ï¼‰ï¼š
  - **Masked å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚**ï¼šé˜²æ­¢çœ‹åˆ°æœªæ¥è¯
  - å¤šå¤´æ³¨æ„åŠ›ï¼ˆå¯¹ç¼–ç å™¨è¾“å‡ºçš„ cross-attentionï¼‰
  - å‰é¦ˆç½‘ç»œ
  - å±‚å½’ä¸€åŒ–ä¸æ®‹å·®è¿æ¥
- ç›®æ ‡ï¼šé€æ­¥ç”Ÿæˆç›®æ ‡è¯­è¨€çš„å•è¯åºåˆ—

<img src="Transformer.assets/image-20250716130718696.png" alt="image-20250716130718696" style="zoom:50%;" />

- **å·¦è¾¹ä¸ºç¼–ç å™¨ï¼Œå³è¾¹ä¸ºè§£ç å™¨**
- Nxï¼šé‡å¤Næ¬¡çš„æ„æ€

- ç¼–ç å™¨çš„è¾“å‡ºä¼šä½œä¸ºè§£ç å™¨çš„ä¸€ä¸ªè¾“å…¥æ”¾å…¥


### encoder

- 6ä¸ªå—ç»„æˆï¼Œæ¯ä¸ªå—æœ‰ä¸¤ä¸ªå­å±‚ï¼ˆsub-layerï¼‰ã€‚
- ç¬¬ä¸€ä¸ªå­å±‚ï¼šmulti-head self-attention mechanism
- ç¬¬äºŒä¸ªå­å±‚ï¼šsimple, position wise fully connected feed-forward networkå°±æ˜¯ä¸€ä¸ªMLP
- æ¯ä¸ªå­å±‚ç”¨ä¸€ä¸ªæ®‹å·®è¿æ¥å’Œä¸€ä¸ª layer normalization
  - layer normalizationï¼š
- å­å±‚å…¬å¼ï¼š$LayerNorm(x + Sublayer(x))$
- ä¸ºäº†ç®€å•ï¼Œæ¯ä¸€å±‚çš„è¾“å‡ºç»´åº¦$d_{model} = 512$

****

`LayerNorm`ï¼ˆå±‚å½’ä¸€åŒ–ï¼‰å’Œ `BatchNorm`ï¼ˆæ‰¹å½’ä¸€åŒ–ï¼‰éƒ½æ˜¯**ç¥ç»ç½‘ç»œä¸­çš„å½’ä¸€åŒ–æŠ€æœ¯**ï¼Œç”¨äºæé«˜è®­ç»ƒç¨³å®šæ€§ï¼ŒåŠ é€Ÿæ”¶æ•›ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜ã€‚å®ƒä»¬çš„æœ¬è´¨ç›®æ ‡ç›¸åŒï¼Œä½†**å½’ä¸€åŒ–çš„ç»´åº¦å’Œåº”ç”¨åœºæ™¯**æœ‰æ‰€ä¸åŒã€‚

#### Batch Normalizationï¼ˆæ‰¹å½’ä¸€åŒ–ï¼‰

> åœ¨ä¸€ä¸ª mini-batch ä¸­ï¼Œå¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦ï¼ˆchannelï¼‰è¿›è¡Œå½’ä¸€åŒ–

é€‚ç”¨åœºæ™¯ï¼š

* **CNNã€MLP ç­‰å‰é¦ˆç½‘ç»œ**
* é€šå¸¸åœ¨å·ç§¯å±‚æˆ–å…¨è¿æ¥å±‚åä½¿ç”¨ï¼ˆ+ ReLUï¼‰

è®¡ç®—å…¬å¼ï¼š

å¯¹äºæ¯ä¸€å±‚è¾“å…¥ $x \in \mathbb{R}^{(N, C, H, W)}$ï¼ˆå¦‚å·ç§¯å±‚è¾“å‡ºï¼‰ï¼š

* **å‡å€¼ï¼ˆæŒ‰ batch ç»´åº¦ï¼‰**ï¼š

  $$
  \mu_c = \frac{1}{N \cdot H \cdot W} \sum_{n,h,w} x_{n,c,h,w}
  $$
* **æ–¹å·®**ï¼š

  $$
  \sigma_c^2 = \frac{1}{N \cdot H \cdot W} \sum_{n,h,w} (x_{n,c,h,w} - \mu_c)^2
  $$
* **å½’ä¸€åŒ–**ï¼š

  $$
  \hat{x}_{n,c,h,w} = \frac{x_{n,c,h,w} - \mu_c}{\sqrt{\sigma_c^2 + \epsilon}}
  $$
* **ç¼©æ”¾ä¸åç§»**ï¼ˆå¯å­¦ä¹ å‚æ•°ï¼‰ï¼š

  $$
  y = \gamma_c \cdot \hat{x}_{n,c,h,w} + \beta_c
  $$

ç‰¹ç‚¹ï¼š

* åˆ©ç”¨äº†æ•´ä¸ª batch çš„ç»Ÿè®¡é‡
* å¯¹ batch å¤§å°æ•æ„Ÿï¼Œå° batch æ•ˆæœå·®
* è®­ç»ƒå’Œæ¨ç†è¡Œä¸ºä¸åŒï¼ˆæ¨ç†ä¸­ç”¨æ»‘åŠ¨å¹³å‡ï¼‰

#### Layer Normalizationï¼ˆå±‚å½’ä¸€åŒ–ï¼‰

> å¯¹å•ä¸ªæ ·æœ¬çš„æ‰€æœ‰ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œä¸ä¾èµ– batch

é€‚ç”¨åœºæ™¯ï¼š

* **RNNã€Transformer ç­‰åºåˆ—æ¨¡å‹**
* ä¸å— batch å¤§å°å½±å“

è¾“å…¥å½¢çŠ¶ï¼ˆå¦‚ Transformer ä¸­çš„ token è¡¨ç¤ºï¼‰ï¼š

$x \in \mathbb{R}^{(B, T, D)}$ï¼Œå…¶ä¸­ D æ˜¯ç‰¹å¾ç»´åº¦

è®¡ç®—å…¬å¼ï¼š

å¯¹æ¯ä¸€ä¸ªæ ·æœ¬ã€æ¯ä¸€ä¸ª token çš„ D ç»´ç‰¹å¾ï¼š

* **å‡å€¼**ï¼š

  $$
  \mu = \frac{1}{D} \sum_{i=1}^{D} x_i
  $$
* **æ–¹å·®**ï¼š

  $$
  \sigma^2 = \frac{1}{D} \sum_{i=1}^{D} (x_i - \mu)^2
  $$
* **å½’ä¸€åŒ–ä¸å˜æ¢**ï¼š

  $$
  \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}, \quad y_i = \gamma_i \cdot \hat{x}_i + \beta_i
  $$

ç‰¹ç‚¹ï¼š

* å½’ä¸€åŒ–ä»…å¯¹å•ä¸ªæ ·æœ¬ï¼Œé€‚ç”¨äº RNN å’Œ Transformer
* ä¸ä¾èµ– batch å¤§å°ï¼Œè®­ç»ƒæ¨ç†ä¸€è‡´
* è®¡ç®—æ•ˆç‡é«˜ï¼Œé€‚ç”¨äºè‡ªæ³¨æ„åŠ›ç»“æ„

#### å¯¹æ¯”æ€»ç»“è¡¨æ ¼

| å¯¹æ¯”é¡¹              | **BatchNorm**                 | **LayerNorm**         |
| ------------------- | ----------------------------- | --------------------- |
| å½’ä¸€åŒ–ç»´åº¦          | Batch + ç‰¹å¾ç»´                | å•ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´      |
| æ˜¯å¦ä¾èµ– Batch å¤§å° | âœ… æ˜¯                          | âŒ å¦                  |
| åº”ç”¨åœºæ™¯            | CNNã€MLP ç­‰                   | RNNã€Transformer      |
| æ˜¯å¦å¯å¹¶è¡Œ          | âŒ å¦ï¼ˆè¦ç­‰æ•´ä¸ª batchï¼‰        | âœ… æ˜¯                  |
| æ¨ç†è¡Œä¸º            | ä¸è®­ç»ƒä¸åŒï¼ˆéœ€ä¿å­˜å‡å€¼/æ–¹å·®ï¼‰ | ä¸è®­ç»ƒä¸€è‡´            |
| ç¼ºç‚¹                | å° batch æ•ˆæœå·®               | å¯¹å›¾åƒå¯èƒ½æ•ˆæœä¸å¦‚ BN |
| PyTorch æ¥å£        | `nn.BatchNorm1d/2d/3d`        | `nn.LayerNorm`        |

---

#### Transformer ä¸­ä¸ºä»€ä¹ˆç”¨ LayerNorm è€Œä¸æ˜¯ BatchNormï¼Ÿ

* Transformer çš„æ¯ä¸ª token æ˜¯ç‹¬ç«‹å¤„ç†çš„ â†’ ä¸ä¾èµ– batch å½’ä¸€åŒ–
* è‡ªæ³¨æ„åŠ›æœºåˆ¶å¤©ç„¶æ”¯æŒå¹¶è¡Œ â†’ LayerNorm æ›´é€‚åˆ
* BatchNorm åœ¨å° batchï¼ˆå¦‚æ¨ç†æ—¶ï¼‰ä¼šé€€åŒ–
* LayerNormç¨³ç‚¹ä¸€äº›

Transformer ä½¿ç”¨ LayerNorm ä¸€èˆ¬åœ¨ä¸¤ä¸ªä½ç½®ï¼š

* åœ¨å­å±‚çš„å‰æˆ–åï¼ˆPost-LN / Pre-LNï¼‰
* å¦‚ï¼š`x = LayerNorm(x + SubLayer(x))`ï¼ˆæ®‹å·®è¿æ¥åå½’ä¸€åŒ–ï¼‰

#### å°ç»“å£è¯€ï¼ˆè®°å¿†ï¼‰

> **BatchNorm ç”¨åœ¨ CNNï¼ŒLayerNorm ç”¨åœ¨ Transformerã€‚**
>
> Batch çœ‹é€šé“ï¼ŒLayer çœ‹è‡ªå·±ã€‚

<img src="Transformer.assets/image-20250716135108377.png" alt="image-20250716135108377" style="zoom:50%;" />

- **Batch**ï¼šä¸€æ¬¡å¤„ç†å‡ å¥è¯
- **Seq**ï¼šæ¯å¥è¯æœ‰å‡ ä¸ªè¯ï¼ˆtokenï¼‰è¿™ä¸ªè¯æ•°é‡å¯èƒ½ä¸åŒï¼Œä¸€èˆ¬ç”¨0è¡¥å…¨
- **Feature**ï¼šæ¯ä¸ªè¯ç”¨å¤šå°‘ç»´å‘é‡æ¥è¡¨ç¤ºï¼ˆ512ï¼‰

- **BatchNorm**
  - è“è‰²åˆ‡ï¼Œç„¶ååšnormï¼ˆå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼‰
- **LayerNorm**
  - é»„è‰²åˆ‡ï¼Œç„¶ååšnormï¼ˆå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼‰

### decoder

- 6å±‚å—å †å 
- 3ä¸ªå­å±‚
  - ç¬¬ä¸€ä¸ªï¼šå¸¦æ©ç çš„æ³¨æ„åŠ›æœºåˆ¶
    - ä½¿å…¶åœ¨è®­ç»ƒæ—¶ï¼ˆè¾“å…¥æ—¶ï¼‰ä»…çœ‹åˆ° t ä¹‹å‰çš„è¾“å…¥ï¼Œè€Œçœ‹ä¸åˆ° t ä¹‹åçš„è¾“å…¥
    - ä¿è¯åœ¨è®­ç»ƒå’Œé¢„æµ‹æ˜¯è¡Œä¸ºä¸€è‡´
  - ç¬¬äºŒä¸ªï¼šå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
  - ç¬¬ä¸‰ä¸ªï¼šsimple, position wise fully connected feed-forward networkå°±æ˜¯ä¸€ä¸ªMLP
  - ä½¿ç”¨æ®‹å·®å’ŒLayerNormè¿æ¥

### Attention

æ³¨æ„åŠ›æœºåˆ¶ï¼ˆ**Attention Mechanism**ï¼‰æ˜¯æ·±åº¦å­¦ä¹ ä¸­ä¸€ç§**æ¨¡æ‹Ÿâ€œäººç±»æ³¨æ„åŠ›â€çš„æœºåˆ¶**ï¼Œå®ƒå¯ä»¥åœ¨å¤„ç†åºåˆ—æˆ–ç»“æ„åŒ–æ•°æ®æ—¶ï¼Œ**åŠ¨æ€åœ°å…³æ³¨è¾“å…¥ä¸­çš„å…³é”®ä¿¡æ¯**ã€‚

---

#### æ³¨æ„åŠ›æœºåˆ¶

> åœ¨å¤„ç†ä¸€ä¸ªè¾“å…¥åºåˆ—æ—¶ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰ä¿¡æ¯éƒ½åŒç­‰é‡è¦ã€‚
> æ³¨æ„åŠ›æœºåˆ¶è®©æ¨¡å‹å¯ä»¥ **æ ¹æ®ä»»åŠ¡éœ€è¦ï¼Œè‡ªä¸»å†³å®šâ€œå…³æ³¨è°ã€å¿½ç•¥è°â€ã€‚**

---

##### åŸºæœ¬æ•°å­¦å½¢å¼ï¼ˆä»¥æœ€å¸¸è§çš„ **Scaled Dot-Product Attention** ä¸ºä¾‹ï¼‰

ç»™å®šä¸‰ä¸ªçŸ©é˜µï¼š

| çŸ©é˜µ          | å«ä¹‰                     |
| ------------- | ------------------------ |
| **Q (Query)** | æŸ¥è¯¢å‘é‡ï¼ˆæˆ‘æƒ³æ‰¾çš„ä¿¡æ¯ï¼‰ |
| **K (Key)**   | é”®å‘é‡ï¼ˆä½ æœ‰ä»€ä¹ˆä¿¡æ¯ï¼‰   |
| **V (Value)** | å€¼å‘é‡ï¼ˆå…·ä½“è¦è¿”å›ä»€ä¹ˆï¼‰ |

##### ğŸ¯ Attention(Q, K, V) çš„å…¬å¼ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

å„éƒ¨åˆ†è§£é‡Šï¼š

* $QK^T$ï¼šè®¡ç®—æŸ¥è¯¢å’Œé”®ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼‰
* $\sqrt{d_k}$ï¼šé˜²æ­¢æ•°å€¼è¿‡å¤§å¯¼è‡´softmaxåå‘ä¸¤è¾¹é æ‹¢è€Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼ˆåšç¼©æ”¾ï¼‰
* `softmax`ï¼šå°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºæƒé‡ï¼ˆæ€»å’Œä¸º1ï¼‰
* ä¸ $V$ ç›¸ä¹˜ï¼šåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°è¾“å‡ºç»“æœ
* çŸ©é˜µè¿ç®—
  * <img src="Transformer.assets/image-20250716155009299.png" alt="image-20250716155009299" style="zoom:50%;" />


---

##### ç›´è§‚ç†è§£ï¼ˆä¸¾ä¾‹ï¼‰

å‡è®¾ä½ æ­£åœ¨ç¿»è¯‘å¥å­ï¼š

> â€œ**The cat sat on the mat.**â€

åœ¨ç¿»è¯‘ â€œsatâ€ æ—¶ï¼š

* Query æ˜¯â€œsatâ€è¿™ä¸ªè¯çš„è¡¨ç¤ºï¼›
* æ¨¡å‹å¯èƒ½é‡ç‚¹å…³æ³¨ â€œcatâ€ï¼ˆä¸»è¯­ï¼‰ï¼Œä¹Ÿå…³æ³¨â€œmatâ€ï¼ˆåœ°ç‚¹ï¼‰ï¼›
* **æ³¨æ„åŠ›æœºåˆ¶ä¼šç»™è¿™äº›è¯åˆ†é…è¾ƒå¤§çš„æƒé‡**ï¼Œä»è€Œå¸®åŠ©æ›´å‡†ç¡®åœ°ç¿»è¯‘â€œsatâ€ã€‚

---

##### Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰

> æ¯ä¸ª token åŒæ—¶æ˜¯ Queryã€Keyã€Valueï¼Œå¯¹æ•´ä¸ªåºåˆ—å†…éƒ¨â€œè‡ªæˆ‘å…³æ³¨â€ã€‚

* å…è®¸æ¨¡å‹åœ¨æ¯ä¸€å±‚ä¸­ç†è§£ï¼š
  æŸä¸ªè¯åº”è¯¥æ³¨æ„å¥å­ä¸­å“ªäº›å…¶å®ƒè¯ã€‚
* æ˜¯ Transformer çš„æ ¸å¿ƒæœºåˆ¶ï¼

---

##### Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰

> ä¸æ˜¯åªè®¡ç®—ä¸€æ¬¡æ³¨æ„åŠ›ï¼Œè€Œæ˜¯**å¤šä¸ªâ€œå¤´â€å¹¶è¡Œå­¦ä¹ ä¸åŒçš„å…³æ³¨æ¨¡å¼**

* å¤šå¤´å…è®¸æ¨¡å‹åœ¨ä¸åŒçš„â€œå­ç©ºé—´â€ä¸Šå…³æ³¨ä¸åŒç‰¹å¾ï¼š

  * ä¸€ä¸ªå¤´å¯èƒ½å…³æ³¨ä¸»è¯­-è°“è¯­
  * ä¸€ä¸ªå¤´å¯èƒ½å…³æ³¨ä»£è¯æŒ‡ä»£
  * ä¸€ä¸ªå¤´å…³æ³¨å¥æ³•å…³ç³»

ğŸ“ å…¬å¼ç»“æ„ï¼š

```text
MultiHead(Q,K,V) = Concat(headâ‚, ..., headâ‚•) Â· W_o
where headáµ¢ = Attention(QWáµ¢^Q, KWáµ¢^K, VWáµ¢^V)
```

---

##### æ³¨æ„åŠ›æœºåˆ¶åœ¨ä¸åŒæ¨¡å‹ä¸­çš„åº”ç”¨

| æ¨¡å‹                   | ä½¿ç”¨æ–¹å¼                      | ä¸¾ä¾‹                      |
| ---------------------- | ----------------------------- | ------------------------- |
| Transformer            | è‡ªæ³¨æ„åŠ› + å¤šå¤´æ³¨æ„åŠ›         | æ¯å±‚éƒ½ç”¨                  |
| BERT                   | ç¼–ç å™¨å †å  + å¤šå¤´æ³¨æ„åŠ›       | ç†è§£ä»»åŠ¡ï¼Œå¦‚é—®ç­”ã€åˆ†ç±»    |
| GPT                    | è§£ç å™¨å †å  + Masked Attention | æ–‡æœ¬ç”Ÿæˆ                  |
| å›¾åƒTransformerï¼ˆViTï¼‰ | ç”¨æ³¨æ„åŠ›ä»£æ›¿å·ç§¯              | å›¾åƒåˆ†ç±»                  |
| Seq2Seqï¼ˆç¿»è¯‘æ¨¡å‹ï¼‰    | ç¼–ç å™¨-è§£ç å™¨ Attention       | Decoder å…³æ³¨ Encoder è¾“å‡º |

---

##### æ€»ç»“å£è¯€

> **æ³¨æ„åŠ›æœºåˆ¶å°±æ˜¯ï¼šè®¡ç®—ç›¸å…³æ€§ + åŠ æƒç»„åˆä¿¡æ¯**
>
> â€œQuery æƒ³çŸ¥é“ä»€ä¹ˆï¼ŒKey æä¾›çº¿ç´¢ï¼ŒValue ç»™å‡ºç­”æ¡ˆâ€

---

#### Scaled Dot-Product Attention

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

è¿™é‡Œé™¤ä»¥$\sqrt{d_k}$ä½œç”¨

- dkè¿‡å¤§æ—¶ï¼Œç‚¹ä¹˜ä¸­å€¼ç›¸å¯¹çš„å·®è·å°±ä¼šå˜å¤§ï¼Œå¯¼è‡´å€¼å¤§çš„softmaxæ›´é è¿‘1ï¼Œå‰©ä¸‹çš„é è¿‘0ï¼Œå¯¼è‡´æ›´åŠ å‘ä¸¤è¾¹é æ‹¢ï¼Œè¿™æ ·æ¢¯åº¦ä¼šå˜å°ï¼Œå¯¼è‡´è·‘ä¸åŠ¨äº†
- æ‰€ä»¥é™¤ä»¥$\sqrt{d_k}$ï¼Œé˜²æ­¢æ•°å€¼è¿‡å¤§å¯¼è‡´softmaxåå‘ä¸¤è¾¹é æ‹¢è€Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±

![](Transformer.assets/image-20250716161700141.png)

Maskï¼šå¯¹äºQtå’ŒKtä¹‹åçš„å€¼æ¢æˆä¸€ä¸ªéå¸¸å¤§çš„è´Ÿæ•°ï¼Œåœ¨softmaxåå°±ä¼šå˜ä¸º0ï¼Œä½¿å¾—åœ¨åšé¢„æµ‹æ—¶è®­ç»ƒæµ‹è¯•çš„æ­¥éª¤ä¸€æ ·

#### Multi-Head Attention

![image-20250716161831981](Transformer.assets/image-20250716161831981.png)

1. åœ¨Linearå±‚åšhæ¬¡æŠ•å½±ï¼Œä½¿å…¶å†ç»è¿‡hæ¬¡æ³¨æ„åŠ›æœºåˆ¶å—
2. ä½¿å…¶æœ‰hæ¬¡æœºä¼šï¼Œå»å­¦ä¹ ä¸ä¸€æ ·çš„æŠ•å½±æ–¹æ³•çš„åº¦é‡ç©ºé—´ä¸­åŒ¹é…ä¸åŒæ¨¡å¼åŠå…¶éœ€è¦çš„ç›¸ä¼¼å‡½æ•°
3. æœ€åæ€»ç»“å†åšä¸€æ¬¡æŠ•å½±

![image-20250716164204527](Transformer.assets/image-20250716164204527.png)

**è¿™é‡Œè®¾ç½® h = 8 ï¼Œä½¿å¾—æŠ•å½±ä¸º $d_k = d_v = d_{model}/h = 64$ï¼Œæœ€ååˆå¹¶æŠ•å½±å›æ¥**

#### Transformerä¸­å¦‚ä½•ä½¿ç”¨Attention

<img src="Transformer.assets/image-20250716130718696.png" alt="image-20250716130718696" />

- **ç¼–ç å™¨è‡ªæ³¨æ„åŠ›å±‚**
- **è§£ç å™¨Maskedè‡ªæ³¨æ„åŠ›å±‚**
- **è§£ç å™¨ç¼–ç å™¨è¿æ¥çš„æ³¨æ„åŠ›å±‚**
  - **key å’Œ value æ¥è‡ªäºç¼–ç å™¨çš„è¾“å‡º**
  - **query æ¥è‡ªäºä¸Šä¸€ä¸ª attention çš„è¾“å‡º**
  - ä½œç”¨ï¼šå°†ç¼–ç å™¨ä¸­æˆ‘æƒ³è¦çš„æœ‰æ•ˆè¾“å‡ºæ‹¿è¿‡æ¥

### Position-wise Feed-Forward Networks

ç¼–ç å™¨å’Œè§£ç å™¨ä¸­çš„æ¯ä¸€å±‚**è¿˜åŒ…å«ä¸€ä¸ªå‰é¦ˆå…¨è¿æ¥ç¥ç»ç½‘ç»œ**ï¼Œè¯¥ç½‘ç»œ**å¯¹æ¯ä¸ªä½ç½®åˆ†åˆ«ä¸”ä¸€è‡´åœ°åº”ç”¨**ã€‚ è¿™ä¸ªå‰é¦ˆç½‘ç»œç”±**ä¸¤ä¸ªçº¿æ€§å˜æ¢**ç»„æˆï¼Œä¸­é—´ä½¿ç”¨äº†**ReLU æ¿€æ´»å‡½æ•°**ã€‚

è™½ç„¶è¿™ä¸¤ä¸ªçº¿æ€§å˜æ¢åœ¨**ä¸åŒçš„ä½ç½®ä¸Š**ä½¿ç”¨çš„æ˜¯**ç›¸åŒçš„å‚æ•°**ï¼ˆå³å‚æ•°åœ¨ **token ä¹‹é—´å…±äº«**ï¼‰ï¼Œä½†åœ¨**ä¸åŒçš„ç½‘ç»œå±‚ä¹‹é—´**ï¼Œè¿™äº›å‚æ•°æ˜¯**ä¸åŒçš„**ï¼ˆæ¯ä¸€å±‚éƒ½æœ‰è‡ªå·±çš„ä¸€å¥— FFN æƒé‡ï¼‰ã€‚

**å¦ä¸€ç§æè¿°æ–¹å¼æ˜¯**ï¼šè¿™ä¸ªå‰é¦ˆç½‘ç»œå¯ä»¥çœ‹ä½œæ˜¯**ä¸¤ä¸ªå·ç§¯æ ¸å¤§å°ä¸º1çš„ä¸€ç»´å·ç§¯æ“ä½œ**ã€‚

**è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦**éƒ½æ˜¯ $d_{\text{model}} = 512$ï¼Œè€Œ**å‰é¦ˆç½‘ç»œå†…éƒ¨çš„éšè—å±‚ç»´åº¦**æ˜¯ $d_{\text{ff}} = 2048$ã€‚

æœ€åæŠ•å½±å›512ï¼Œå› ä¸ºéœ€æœ‰æ®‹å·®è¿æ¥

#### ç»“æ„å½¢å¼

$$
\text{FFN}(x) = \text{max}(0,xW_1 + b_1)W_2 + b_2
$$

#### ç»“æ„å›¾ç†è§£

```text
è¾“å…¥ x âˆˆ â„^{batch_size Ã— seq_len Ã— 512}
     â”‚
     â””â”€â”€â–º Linear(512 â†’ 2048)
               â”‚
               â–¼
           ReLU æ¿€æ´»
               â”‚
               â–¼
     â””â”€â”€â–º Linear(2048 â†’ 512)
               â”‚
               â–¼
         è¾“å‡º x' âˆˆ â„^{batch_size Ã— seq_len Ã— 512}
```

#### æ€»ç»“

> Transformer ä¸­æ¯ä¸€å±‚ï¼ˆä¸è®ºæ˜¯ç¼–ç å™¨è¿˜æ˜¯è§£ç å™¨ï¼‰é™¤äº†æ³¨æ„åŠ›å­å±‚ï¼Œè¿˜åŒ…æ‹¬ä¸€ä¸ª**å‰é¦ˆç¥ç»ç½‘ç»œå­å±‚ï¼ˆFFNï¼‰**ã€‚
> FFN å¯¹æ¯ä¸ªä½ç½®çš„è¡¨ç¤ºå•ç‹¬å¤„ç†ï¼Œç”±ä¸¤å±‚å…¨è¿æ¥ + ReLU æ„æˆï¼ˆ512 â†’ 2048 â†’ 512ï¼‰ã€‚**å‚æ•°åœ¨ token ä¹‹é—´å…±äº«ï¼Œä½†åœ¨ä¸åŒå±‚ä¹‹é—´ä¸å…±äº«**ã€‚

#### ä¸€ä¸ªAttentionå’Œä¸€ä¸ªFeed Forwardä¸¾ä¾‹

<img src="Transformer.assets/image-20250716172524032.png" alt="image-20250716172524032" style="zoom:50%;" />

- è¿™é‡Œä¸‹é¢çš„çº¢è‰²å—æ˜¯Attention
  - ä½¿å¾—**å«æœ‰åºåˆ—ä¿¡æ¯**
- ä¸Šé¢çš„çº¢è‰²å—æ˜¯MLPï¼Œä¸”åœ¨ä¸€ä¸ªå±‚ä¸­tokenä¹‹é—´å…±äº«å³ä¸€æ ·
  - å¯¹æ¯ä¸ªä½ç½®çš„è¡¨ç¤ºè¿›è¡Œ**éçº¿æ€§å˜æ¢å’Œç‰¹å¾å¢å¼º**

### Embeddings and Softmax

embedding

- **å°†è¯å…ƒæ˜ å°„æˆå‘é‡**é•¿åº¦ä¸º$d_{model}$ï¼Œåœ¨æ¨¡å‹ä¸­ä¸åŒåœ°æ–¹çš„embeddingæ˜¯ä¸€æ ·çš„æƒé‡
- **è¿™é‡Œembeddingä¹˜ä»¥**$\sqrt{d_{model}}$ï¼Œå› ä¸ºembeddingä¸ç®¡ d å¤šå¤§æ—¶ï¼Œå€¼ä»ç„¶ä¸º1ï¼Œç»´åº¦ä¸€å¤§åï¼Œå…¶æƒé‡å€¼ä¼šè¿‡å°ï¼Œè€Œ**åé¢éœ€è¦åŠ ä¸ŠPositional Encodingæ‰€ä»¥æ‰©å¤§ä½¿å¾—å…¶åœ¨ç›¸åŠ æ—¶åœ¨ä¸€ä¸ªæ•°é‡çº§ä¸Šå·®ä¸å¤š**

### Positional Encoding

å› ä¸ºattentionæœ¬èº«ä¸ä¼šå»å…³æ³¨å…¶å¥å­ä¸­ä½ç½®å­˜æ”¾çš„ä¿¡æ¯ï¼Œè¿™é‡Œä½¿ç”¨Positonal Encodingåœ¨attentionæœºåˆ¶ä¸­åŠ å…¥æ—¶åºä¿¡æ¯ï¼Œå³å°†ä½ç½®ä¿¡æ¯ä¹Ÿå½“ä½œè¾“å…¥

<img src="Transformer.assets/image-20250716180110675.png" alt="image-20250716180110675" style="zoom: 50%;" />

å°†**æ—¶åºä¿¡æ¯ä¸åµŒå…¥å±‚ç›¸åŠ **ï¼Œå°±å®Œæˆäº†å°†æ—¶åºä¿¡æ¯åŠ å…¥æ•°æ®çš„æ­¥éª¤

## Why Self-Attention

![image-20250716182352302](Transformer.assets/image-20250716182352302.png)

- nï¼šåºåˆ—é•¿åº¦ï¼Œå•è¯æ•°n
- dï¼šç»´åº¦ï¼Œç”¨å¤šå°‘ç»´å‘é‡è¡¨ç¤ºä¸€ä¸ªå•è¯
- kï¼šå·ç§¯çš„æ ¸å¤§å°
- rï¼šé™åˆ¶æ€§è‡ªæ³¨æ„åŠ›ï¼ˆrestricted self-attentionï¼‰ä¸­çš„é‚»åŸŸå¤§å°ã€‚

| ç¬¦å· | å«ä¹‰                                                         | ä¸¾ä¾‹                                                         |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| n    | **sequence length**ï¼šåºåˆ—é•¿åº¦                                | NLP ä¸­ä¸€å¥è¯çš„ token æ•°ï¼Œæ¯”å¦‚ "I love NLP." æ˜¯ 3             |
| d    | **representation dimension**ï¼šè¡¨ç¤ºç»´åº¦                       | æ¯ä¸ªè¯çš„ embedding ç»´åº¦ï¼Œå¦‚ 512ã€768ï¼ˆBERTï¼‰ç­‰               |
| k    | **kernel size**ï¼šå·ç§¯æ ¸å¤§å°                                  | CNN ä¸­å¸¸ç”¨å¤§å°å¦‚ 3ã€5ï¼Œè¡¨ç¤ºå±€éƒ¨çª—å£å¤§å°                      |
| r    | **neighborhood size** in **restricted self-attention**ï¼šé™åˆ¶æ€§æ³¨æ„åŠ›çš„èŒƒå›´ | ä»…è®©ä¸€ä¸ªä½ç½®å…³æ³¨å‰å r ä¸ªä½ç½®ï¼ˆå¦‚å·¦1å³1ï¼‰ï¼Œä¸æ˜¯å…¨å±€ attention |

****

- Complexity per Layerå¤æ‚åº¦
- Sequential Operationsé¡ºåºè®¡ç®—ä¸‹ä¸€æ­¥è®¡ç®—éœ€è¦ç­‰å‰é¢å¤šå°‘æ­¥è®¡ç®—ï¼Œè¶Šä¸‹å¹¶è¡Œåº¦è¶Šé«˜
- Maximum Path Lengthä¿¡æ¯ç»è¿‡è·¯ç¨‹ï¼šä¿¡æ¯ä»ä¸€ä¸ªæ•°æ®ç‚¹èµ°åˆ°å¦å¤–ä¸€ä¸ªæ•°æ®ç‚¹è¦èµ°å¤šè¿œï¼Œè¶ŠçŸ­è¶Šå¥½

## Training

### Training Data and Batching

**è®­ç»ƒæ•°æ®å’Œæ‰¹æ¬¡**

æˆ‘ä»¬åŸºäºåŒ…å«çº¦450ä¸‡å¥å­å¯¹çš„æ ‡å‡†WMT 2014è‹±å¾·è¯­æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚é‡‡ç”¨å­—èŠ‚å¯¹ç¼–ç [3]å¯¹å¥å­è¿›è¡Œç¼–ç ï¼Œè¯¥ç¼–ç ä½¿ç”¨å…±äº«çš„æºè¯­è¨€ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨ï¼ŒåŒ…å«çº¦37000ä¸ªæ ‡è®°ã€‚å¯¹äºè‹±æ³•è¯­æ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨è§„æ¨¡æ›´å¤§çš„WMT2014è‹±æ³•è¯­æ•°æ®é›†ï¼ˆåŒ…å«3600ä¸‡å¥å­ï¼‰ï¼Œå°†æ ‡è®°æ‹†åˆ†ä¸º32000ä¸ªè¯ç‰‡è¯æ±‡è¡¨[38]ã€‚æ ¹æ®è¿‘ä¼¼åºåˆ—é•¿åº¦å¯¹å¥å­å¯¹è¿›è¡Œæ‰¹é‡å¤„ç†ï¼Œæ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡åŒ…å«çº¦25000ä¸ªæºè¯­è¨€æ ‡è®°å’Œ25000ç›®æ ‡è¯­è¨€æ ‡è®°çš„å¥å­å¯¹é›†åˆã€‚ 

****

### Hardware and Schedule

**ç¡¬ä»¶å’Œæ—¶é—´è¡¨**

æˆ‘ä»¬åœ¨é…å¤‡8å—NVIDIA P100 GPUçš„å•æœºä¸Šè®­ç»ƒæ¨¡å‹ã€‚å¯¹äºåŸºç¡€æ¨¡å‹ï¼ˆä½¿ç”¨æ–‡ä¸­æ‰€è¿°çš„è¶…å‚æ•°é…ç½®ï¼‰ï¼Œæ¯ä¸ªè®­ç»ƒæ­¥éª¤è€—æ—¶çº¦0.4ç§’ï¼Œæ€»è®­ç»ƒæ—¶é•¿ä¸º10ä¸‡æ­¥ï¼ˆå³12å°æ—¶ï¼‰ã€‚é’ˆå¯¹å¤§å‹æ¨¡å‹ï¼ˆè¯¦è§è¡¨3åº•éƒ¨è¯´æ˜ï¼‰ï¼Œæ¯æ­¥è€—æ—¶1.0ç§’ï¼Œæ€»è®­ç»ƒæ—¶é•¿è¾¾30ä¸‡æ­¥ï¼ˆç›¸å½“äº3.5å¤©ï¼‰ã€‚

****

### Optimizer

**ä¼˜åŒ–å™¨**

æˆ‘ä»¬ä½¿ç”¨**Adamä¼˜åŒ–å™¨**[20]ï¼Œå‚æ•°è®¾ç½®ä¸º$Î²_1= 0.9ã€Î²_2= 0.98å’ŒÎµ=10^{âˆ’9}$ã€‚

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬**æ ¹æ®ä»¥ä¸‹å…¬å¼è°ƒæ•´å­¦ä¹ ç‡**

<img src="Transformer.assets/image-20250716190721117.png" alt="image-20250716190721117" style="zoom:80%;" />

è¿™å¯¹åº”äºåœ¨å‰ä¸ªé¢„çƒ­è®­ç»ƒæ­¥éª¤ä¸­çº¿æ€§å¢åŠ å­¦ä¹ ç‡ï¼Œä¹‹ååˆ™æŒ‰æ­¥éª¤ç¼–å·çš„å¹³æ–¹æ ¹æ¯”ä¾‹é€’å‡ã€‚ä½¿ç”¨äº†warmup_steps = 4000ã€‚ 

****

### Regularization

**æ­£åˆ™åŒ–**

ä½¿ç”¨**ä¸‰ä¸ª**æ­£åˆ™åŒ–

- **Residual Dropout**ï¼š**æ®‹å·®ä¸¢å¼ƒæœºåˆ¶**ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªå­å±‚çš„è¾“å‡º**åœ¨è¿›å…¥æ®‹å·®è¿æ¥å’ŒLayerNormä¹‹å‰**ï¼Œä¼šä¸¢å¼ƒï¼Œä¸¢å¼ƒç‡ä¸º$P_{drop} = 0.1$ã€‚
- æ­¤å¤–ï¼Œç¼–ç å™¨å’Œè§£ç å™¨å †æ ˆä¸­çš„**åµŒå…¥å‘é‡æ€»å’Œ**åŠ**ä½ç½®ç¼–ç æ€»å’Œ**å‡ä¼š**è¿›è¡Œä¸¢å¼ƒå¤„ç†**ã€‚åŸºç¡€æ¨¡å‹é‡‡ç”¨çš„ä¸¢å¼ƒç‡ä¸º$P_{drop} = 0.1$ã€‚
- **Label Smoothing**ï¼š**æ ‡ç­¾å¹³æ»‘å¤„ç†**ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†**æ ‡ç­¾å¹³æ»‘å€¼ä¸º$Îµ_{ls}=0.1$**[36]ï¼ˆsoftmaxåç½®ä¿¡åº¦ä¸º$1-0.1=0.9$ï¼‰ã€‚è¿™ç§å¤„ç†è™½ç„¶ä¼š**å¢åŠ å›°æƒ‘åº¦**-ï¼ˆå› ä¸ºæ¨¡å‹ä¼šå˜å¾—ä¸é‚£ä¹ˆç¡®å®šï¼‰ï¼Œä½†èƒ½**æœ‰æ•ˆæå‡å‡†ç¡®ç‡å’ŒBLEUåˆ†æ•°**ã€‚ 
  - **æ ‡ç­¾å¹³æ»‘çš„æ„ä¹‰**
    - **ä¸è¦**å¯¹è®­ç»ƒæ ‡ç­¾**å¤ªè‡ªä¿¡**ï¼Œ**å‡å°‘è¿‡æ‹Ÿåˆ**ï¼Œ**æå‡æ³›åŒ–èƒ½åŠ›**ï¼Œç‰¹åˆ«å¯¹æŠ—æ¨¡å‹â€œè¿‡åˆ†ç¡®å®šâ€çš„å€¾å‘æœ‰ç”¨ã€‚
    - å½“**æ ‡ç­¾å¹³æ»‘å€¼ä¸º 0.1** æ—¶ï¼Œæ¨¡å‹å¯¹çœŸå®æ ‡ç­¾çš„ç½®ä¿¡åº¦æ˜¯ **90%**ï¼Œå…¶ä½™ 10% çš„æ¦‚ç‡è¢«**å‡åŒ€åˆ†é…ç»™å…¶ä»–ç±»åˆ«**ã€‚

## ç¼ºç‚¹

- **å¯¹å…³é”®ä¿¡æ¯çš„â€œæŠ“å–èƒ½åŠ›â€å·®**
- **éœ€è¦å¤§é‡æ•°æ®è¿›è¡Œè®­ç»ƒ**
- **è®­ç»ƒèµ„æºæ¶ˆè€—å¤§ï¼ˆæ—¶é—´ + æ˜¾å­˜ + è®¡ç®—ï¼‰**
- **è§£é‡Šæ€§å·®**



## QKV&multi

Vision Transformer (ViT) æ˜¯ Transformer æ¨¡å‹é’ˆå¯¹è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æ”¹ç¼–ã€‚æ‰€ä»¥æˆ‘é€‰æ‹©å…ˆè¿›è¡ŒTransformer æ¨¡å‹çš„å­¦ä¹ 

è¿™é‡Œå‚ç…§[csdnåšå®¢:ä¸€æ–‡è¯»æ‡‚Transformer](https://blog.csdn.net/weixin_42475060/article/details/121101749?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522159E0F39-AD8E-416D-AA92-7DD8F7B0E841%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=159E0F39-AD8E-416D-AA92-7DD8F7B0E841&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~hot_rank-3-121101749-null-null.nonecase&amp;utm_term=Transformer%20%E6%A8%A1%E5%9E%8B%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD&amp;spm=1018.2226.3001.4450 )

Transformeræ˜¯ä¸€ç§ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œå…¶ä»–åºåˆ—åˆ°åºåˆ—ï¼ˆsequence-to-sequenceï¼‰ä»»åŠ¡çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„

æœ€é‡è¦çš„å°±æ˜¯å¼•å…¥äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå°±æ˜¯å°†è¾“å…¥çš„åºåˆ—ä¸åŒéƒ¨åˆ†è¿›è¡Œèµ‹æƒé‡ï¼Œä»è€Œè¾¾åˆ°æ„å»ºè¾“å…¥çš„æ ·æœ¬å†…éƒ¨å½¢æˆäº†å…³ç³»

### æ•´ä½“ç»“æ„

![img](Transformer.assets/3319e3d6922a2e7f2499a3130d3b5925.png)

Encoder blockå¯¹åº”ç¼–ç å™¨

Decoder blockå¯¹åº”è§£ç å™¨

<img src="Transformer.assets/20c5baff36eedc6100d9f107e4fe3c95.png" alt="img" style="zoom:50%;" />

è¿™é‡Œæœ‰å…­ä¸ªç¼–ç å™¨å åŠ ï¼Œ6ä¸ªè§£ç å™¨å åŠ ï¼ˆäº’ç›¸é—´æ²¡æœ‰å…±äº«å‚æ•°ï¼‰

ç¼–ç å™¨ä¸è§£ç å™¨ä¹‹é—´çš„ç®€ç•¥ç»“æ„

<img src="Transformer.assets/630deb7da181d99eb9dd7d70f6b4da98.png" alt="img" style="zoom:50%;" />

è¾“å…¥çš„å¥å­å…ˆè¿›å…¥è‡ªæ³¨æ„å±‚ï¼Œå°†æ¯ä¸ªå•è¯ç¼–ç æ—¶ä¹Ÿä¸å…¶ä»–å•è¯ç›¸å…³è”

è§£ç å™¨çš„æ³¨æ„åŠ›å±‚ä¹Ÿæ˜¯å…³æ³¨æ•´ä¸ªè¾“å…¥çš„ç›¸å…³éƒ¨åˆ†

### Encoder Block

#### å•å¤´è‡ªæ³¨æ„åŠ›å±‚

æˆ‘è®¤ä¸ºæ³¨æ„åŠ›å¤´å°±æ˜¯ä¸ºæ¯ä¸€ä¸ªæ ·æœ¬éƒ½åˆ›å»ºä¸‰ä¸ªæƒé‡çŸ©é˜µï¼Œæ³¨æ„åŠ›æœºåˆ¶ä¼šå°†è¿™äº›æƒé‡çŸ©é˜µåº”ç”¨äºè¾“å…¥ç‰¹å¾ï¼Œç”ŸæˆæŸ¥è¯¢ã€é”®å’Œå€¼ï¼Œè¿›è€Œè®¡ç®—æ³¨æ„åŠ›æƒé‡å¹¶æ•´åˆä¿¡æ¯ï¼Œä»¥æ­¤æ¥å»ºç«‹æ ·æœ¬å†…éƒ¨çš„å…³ç³»è¿æ¥ï¼Œæ•æ‰æ ·æœ¬å†…éƒ¨çš„å¤æ‚å…³ç³»ã€‚

<img src="Transformer.assets/49c88545fae57dbf255c8ab9fd279110.png" alt="img" style="zoom:50%;" />

##### è®¡ç®—æ­¥éª¤

step1ï¼šå…ˆå°†è¾“å…¥å‘é‡è¿›è¡Œè¯åµŒå…¥ä¸ä¸‰ä¸ªæƒé‡çŸ©é˜µè¿›è¡Œç›¸ä¹˜åˆ›å»ºå‡ºæŸ¥è¯¢å‘é‡Qï¼Œé”®å‘é‡Kï¼Œå€¼å‘é‡V

<img src="Transformer.assets/cc00beb97c344a486d07e3d9e8a58f06.png" alt="img" style="zoom:50%;" />

step2ï¼šè®¡ç®—è‡ªæ³¨æ„åŠ›å±‚çš„è¾“å‡ºã€‚

<img src="Transformer.assets/00994ceb6bf9e66db19611c496463364.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" style="zoom:50%;" />

æ•´ä½“çš„è®¡ç®—å›¾

<img src="Transformer.assets/e976d386a1aad85c2efb7fc965099c27.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" style="zoom:50%;" />

1.å…ˆå°†xä¸Wqã€Wkã€Wvè¿›è¡ŒçŸ©é˜µç›¸ä¹˜å¾—å‡ºæŸ¥è¯¢å‘é‡ã€é”®å‘é‡ã€å€¼å‘é‡

2.æŸ¥è¯¢å‘é‡*é”®å‘é‡å¾—å‡ºåˆ†æ•°

3.åˆ†æ•°é™¤ä»¥dkçš„å¹³æ–¹æ ¹

4.å¯¹ç¬¬ä¸‰æ­¥çš„å¾—åˆ†è¿›è¡Œsoftmaxå½’ä¸€åŒ–

4.softmaxåçš„å€¼*å€¼å‘é‡æ±‚å’Œåå°±æ˜¯è¯¥å•è¯åœ¨è¯¥å¥å­ä¸­çš„æ³¨æ„åŠ›äº†

#### å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚Multiâ€”â€”Head Attention

Multiæ˜¯å°†è¿™ä¸ªå…³ç³»å»ºç«‹å¾—æ›´åŠ æ¸…æ™°ï¼Œæ¯ä¸ªå¤´éƒ½æœ‰è‡ªå·±çš„ä¸€ç»„æƒé‡çŸ©é˜µã€‚

å…³é”®æ˜¯åˆ†å¤´

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](Transformer.assets/a923f7bb907110650448d4b773bf0671.png)

åˆ†å¤´çš„å®ç°ï¼šç”¨å¤šç»„Wqï¼ŒWkï¼ŒWvå¾—åˆ°å¤šç»„æŸ¥è¯¢ã€é”®ã€å€¼çŸ©é˜µï¼Œç„¶åæ¯ç»„åˆ†åˆ«è®¡ç®—å¾—åˆ°ä¸€ä¸ªZçŸ©é˜µã€‚æœ€åå½¢æˆå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œç„¶åå†ç”¨ä¸€ä¸ªçŸ©é˜µå°†å¤šå¤´æ‹¼æ¥ï¼Œæœ€åå†ä¸ä¸€ä¸ªå»ºç«‹çš„é™„åŠ æƒé‡çŸ©é˜µç›¸ä¹˜æœ€åå¾—åˆ°ä¸€ä¸ªæ³¨æ„åŠ›å¤´çŸ©é˜µï¼ˆç›¸å½“äºåˆæˆæ•°æ®ï¼‰

<img src="Transformer.assets/fd6af04ca65df88d3f13f4aaf987b0f3.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" style="zoom:50%;" />

é€šè§ˆ

![](Transformer.assets/0b8dacfc201e24ef7dc0e690b41b998c.png)

#### ä½ç½®ç¼–ç ä½¿ç”¨

ä½ç½®ç¼–ç çš„å­˜åœ¨æ˜¯ä½¿åŒä¸€ä¸ªå•è¯åœ¨ä¸åŒä½ç½®çš„æ³¨æ„åŠ›åˆ†æ•°ä¸ä¸€æ ·

è¿™é‡Œé€‰æ‹©è¯åµŒå…¥ä¸ä½ç½®ç¼–ç è¿›è¡Œç›¸åŠ ï¼Œä¸æ˜¯æ‹¼æ¥ï¼ˆè¿™é‡Œæ˜¯é˜²æ­¢ç»´åº¦å¢åŠ ï¼‰

<img src="Transformer.assets/{A8129329-470C-42EB-B584-421725167A39}.png" alt="img" style="zoom:50%;" />

#### Add&Normalize

åœ¨ç»è¿‡å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å¾—åˆ°çŸ©é˜µZåï¼ŒZå¹¶æ²¡æœ‰ç›´æ¥ä¼ å…¥å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œè€Œæ˜¯éœ€è¦ç»è¿‡ä¸€æ­¥Add&Normalizeã€‚

<img src="Transformer.assets/29a24a78b70aa77ffd41b5ae2bfdc5e7.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" style="zoom:50%;" />

Addï¼šåœ¨zçš„åŸºç¡€ä¸ŠåŠ äº†ä¸€ä¸ªæ®‹å·®å—Xï¼Œé˜²æ­¢åœ¨æ·±åº¦ç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé€€åŒ–çš„é—®é¢˜ï¼Œé€€åŒ–çš„æ„æ€å°±æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œé€šè¿‡å¢åŠ ç½‘ç»œçš„å±‚æ•°ï¼ŒLossé€æ¸å‡å°ï¼Œç„¶åè¶‹äºç¨³å®šè¾¾åˆ°é¥±å’Œï¼Œç„¶åå†ç»§ç»­å¢åŠ ç½‘ç»œå±‚æ•°ï¼ŒLossåè€Œå¢å¤§ã€‚

Normalizeï¼šå½’ä¸€åŒ–ï¼ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦ã€æé«˜è®­ç»ƒçš„ç¨³å®šæ€§

è¿™é‡Œé€‰ç”¨çš„æ˜¯LNå¯ä»¥æ¯ä¸€ç»´ä¸Šè¿›è¡Œå½’ä¸€åŒ–

#### Feed Forwardï¼ˆå…¨è¿æ¥å±‚ï¼‰

![](Transformer.assets/98f4c96bfa951f24b2d1b8c8686582bd.png)

åœ¨è¿™é‡Œå…¨è¿æ¥å±‚æ˜¯ä¸€ä¸ª**ä¸¤å±‚çš„ç¥ç»ç½‘ç»œ**ï¼Œå…ˆçº¿æ€§å˜æ¢ï¼Œç„¶åReLUéçº¿æ€§ï¼Œå†çº¿æ€§å˜æ¢ã€‚

### Decoder Block

![](Transformer.assets/da0673543394c7623497447b1211054d.png)

Decoderä¹Ÿæ˜¯ç”±6ä¸ªdecoderå †å è€Œæˆçš„ã€‚ä¸€ä¸ªDecoderåŒ…å«ä¸¤ä¸ª Multi-Head Attention å±‚ã€‚ç¬¬ä¸€ä¸ª Multi-Head Attention å±‚é‡‡ç”¨äº† Masked æ“ä½œã€‚ç¬¬äºŒä¸ª Multi-Head Attention å±‚çš„K, VçŸ©é˜µä½¿ç”¨ Encoder çš„ç¼–ç ä¿¡æ¯çŸ©é˜µCè¿›è¡Œè®¡ç®—ï¼Œè€ŒQä½¿ç”¨ä¸Šä¸€ä¸ª Decoder block çš„è¾“å‡ºè®¡ç®—ã€‚

#### Masked Multi-Head Attention

ä¸Encoderçš„Multi-Head Attentionè®¡ç®—åŸç†ä¸€æ ·ï¼Œåªæ˜¯å¤šåŠ äº†ä¸€ä¸ªmaskç ã€‚mask è¡¨ç¤ºæ©ç ï¼Œå®ƒå¯¹æŸäº›å€¼è¿›è¡Œæ©ç›–ï¼Œä½¿å…¶åœ¨å‚æ•°æ›´æ–°æ—¶ä¸äº§ç”Ÿæ•ˆæœã€‚

### è¾“å‡º

é¦–å…ˆç»è¿‡ä¸€æ¬¡çº¿æ€§å˜æ¢ï¼Œç„¶åSoftmaxå¾—åˆ°è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒï¼Œç„¶åé€šè¿‡è¯å…¸ï¼Œè¾“å‡ºæ¦‚ç‡æœ€å¤§çš„å¯¹åº”çš„å•è¯ä½œä¸ºæˆ‘ä»¬çš„é¢„æµ‹è¾“å‡ºã€‚

## å›¾åƒç†è§£

[ã€å®˜æ–¹åŒè¯­ã€‘ç›´è§‚è§£é‡Šæ³¨æ„åŠ›æœºåˆ¶ï¼ŒTransformerçš„æ ¸å¿ƒ | ã€æ·±åº¦å­¦ä¹ ç¬¬6ç« ã€‘](https://www.bilibili.com/video/BV1TZ421j7Ke?vd_source=64fa735df4e10c3811ddac775f3035f1)

![image-20250827152548992](Transformer.assets/image-20250827152548992.png)

![image-20250827152623220](Transformer.assets/image-20250827152623220.png)

Transformerä½¿å…¶ä¸å•å•ç¼–ç å•ä¸ªè¯ï¼Œè¿˜èƒ½èå…¥æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡å«ä¹‰

![image-20250827152801389](Transformer.assets/image-20250827152801389.png)

æ¨¡å‹ç›¸äº’ä¼ é€’åµŒå…¥å‘é‡è•´å«çš„ä¿¡æ¯è¿›è¡Œæ›´æ–°

![image-20250827153055423](Transformer.assets/image-20250827153055423.png)

é¢„æµ‹ä¸‹ä¸€ä¸ªtokençš„è®¡ç®—è¿‡ç¨‹å®Œå…¨åŸºäºåºåˆ—ä¸­æœ€åä¸€ä¸ªå‘é‡

![image-20250827153259742](Transformer.assets/image-20250827153259742.png)

åŸå§‹å‘é‡ï¼ˆåµŒå…¥å‘é‡ï¼‰åŒ…å«è¯æœ¬èº«ä»¥åŠå®ƒåœ¨æ–‡ä¸­ä½ç½®ä¿¡æ¯

![image-20250827153545520](Transformer.assets/image-20250827153545520.png)

æœ€ç»ˆç›®æ ‡è®¡ç®—å‡ºå…¶å®é™…è¡¨ç¤ºçš„æ„ä¹‰å¯¹åº”å‘é‡

- æ¯”å¦‚ç»è¿‡å½¢å®¹è¯ä¿®é¥°åå¯¹åº”çš„å‘é‡

![image-20250827153637037](Transformer.assets/image-20250827153637037.png)

### Single head of attention

#### Q æŸ¥è¯¢çŸ©é˜µ

![image-20250827153945836](Transformer.assets/image-20250827153945836.png)

æŸ¥è¯¢å‘é‡ç›®çš„æä¾›æƒé‡å»æ³¨æ„æœ‰å…³tokençš„å‘é‡

æŸ¥è¯¢å‘é‡ç»´åº¦æ¯”åµŒå…¥å‘é‡å°å¾—å¤š

![image-20250827154357390](Transformer.assets/image-20250827154357390.png)

![image-20250827154516991](Transformer.assets/image-20250827154516991.png)

æ³¨æ„ï¼šè¿™é‡Œå‡è®¾æŸ¥è¯¢çŸ©é˜µå…³æ³¨çš„æ˜¯å½¢å®¹è¯

#### K é”®çŸ©é˜µ

**å°† [é”®] è§†ä¸ºæƒ³è¦å›ç­” [æŸ¥è¯¢]**

![image-20250827154830259](Transformer.assets/image-20250827154830259.png)

é”®ä¸æŸ¥è¯¢é‡åˆåº¦è¶Šé«˜å°±è§†ä¸ºåŒ¹é…åº¦è¶Šé«˜

![image-20250827154951259](Transformer.assets/image-20250827154951259.png)

è¿›è¡ŒçŸ©é˜µä¹˜æ³•ä»¥å¾—å‡ºåŒ¹é…åº¦

![image-20250827155109180](Transformer.assets/image-20250827155109180.png)

ç‚¹ç§¯è¶Šå¤§---ã€‹åŸç‚¹è¶Šå¤§---ã€‹é”®ä¸æŸ¥è¯¢è¶Šå¯¹é½---ã€‹tokenåµŒå…¥æ³¨æ„åˆ°äº†æŸä¸ªtokenå¾—åµŒå…¥

é‡‡ç”¨softmaxè¿›è¡Œå½’ä¸€åŒ–

![image-20250827155527110](Transformer.assets/image-20250827155527110.png)

![image-20250827155547881](Transformer.assets/image-20250827155547881.png)

![image-20250827161244411](Transformer.assets/image-20250827161244411.png)

![image-20250827161306795](Transformer.assets/image-20250827161306795.png)

#### æ©ç æœºåˆ¶

ä¸ºäº†ä½¿ä¸€ä¸ªè®­ç»ƒæ ·æœ¬èƒ½æä¾›å¤šæ¬¡è®­ç»ƒæœºä¼šï¼Œéœ€è¦ä½¿å¾—åé¢çš„tokenä¸è¢«çœ‹åˆ°ï¼Œå³åªèƒ½çœ‹åˆ°å‰é¢çš„è¯è¯­ï¼Œä¸èƒ½è®©åé¢çš„tokenå½±å“å‰é¢çš„token

![image-20250827161559349](Transformer.assets/image-20250827161559349.png)

![image-20250827162106075](Transformer.assets/image-20250827162106075.png)

#### V å€¼çŸ©é˜µ

![image-20250827162333445](Transformer.assets/image-20250827162333445.png)

![image-20250827162437198](Transformer.assets/image-20250827162437198.png)

è¡¨ç¤º**è¿™ä¸ªè¯**éœ€è¦**è°ƒæ•´ç›®æ ‡è¯çš„å«ä¹‰**æ—¶éœ€è¦æ·»åŠ çš„å‘é‡

ç´¯åŠ å¾—åˆ°æƒ³è¦å¼•å…¥çš„å˜åŒ–é‡

![image-20250827162851974](Transformer.assets/image-20250827162851974.png)

æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªç¼–ç äº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä¸€ä¸ªæ›´ç²¾å‡†çš„åµŒå…¥å‘é‡

![image-20250827163003665](Transformer.assets/image-20250827163003665.png)

é€šè¿‡æ³¨æ„åŠ›æ¨¡å—ï¼Œå¾—åˆ°äº†ä¸€ç³»åˆ—æ›´ç²¾å‡†çš„åµŒå…¥å‘é‡

#### å‚æ•°é‡

![image-20250827164100954](Transformer.assets/image-20250827164100954.png)

**æ›´é«˜æ•ˆçš„åšæ³•**

- **è®©å€¼çŸ©é˜µæ‰€éœ€å‚æ•°é‡ç­‰äºé”®çŸ©é˜µå’ŒæŸ¥è¯¢çŸ©é˜µçš„å‚æ•°é‡ä¹‹å’Œ**ï¼Œè¿™å¯¹äºå¹¶è¡Œå¤šæ³¨æ„åŠ›å¤´è€Œè¨€éå¸¸é‡è¦

**åšæ³•ï¼šå°†å€¼çŸ©é˜µåˆ†ä¸ºä¸¤ä¸ªå°çŸ©é˜µï¼ˆå¯¹å¤§çŸ©é˜µè¿›è¡Œ ä½ç§©åˆ†è§£ ï¼‰ï¼Œå°†æ•´ä½“è§†ä¸ºä¸€ä¸ªçº¿æ€§æ˜ å°„**

![image-20250827164458805](Transformer.assets/image-20250827164458805.png)

![image-20250827164835529](Transformer.assets/image-20250827164835529.png)

![image-20250827165213419](Transformer.assets/image-20250827165213419.png)

### Multi-headed attention

å¤§é‡ å¹¶è¡Œ

![image-20250827165849371](Transformer.assets/image-20250827165849371.png)

![image-20250827170348827](Transformer.assets/image-20250827170348827.png)

æ€»å’Œæ˜¯è¿™ä¸ªæ¨¡å—çš„è¾“å‡ºï¼Œå³é€šè¿‡è¿™ä¸ªæ¨¡å—å¾—åˆ°çš„æ›´ç²¾ç¡®çš„åµŒå…¥

![image-20250827170449824](Transformer.assets/image-20250827170449824.png)

![image-20250827170533388](Transformer.assets/image-20250827170533388.png)

è®­ç»ƒæ—¶å¯èƒ½ä¼šå°†Value upçŸ©é˜µåˆå¹¶ä¸ºä¸€ä¸ªå¤§çŸ©é˜µï¼ˆç§°ï¼š**è¾“å‡ºçŸ©é˜µ**ï¼‰

- ä¸æ•´ä¸ªå¤šå¤´æ³¨æ„åŠ›æ¨¡å—ç›¸å…³è”

å•ä¸ªæ³¨æ„åŠ›å¤´çš„å€¼çŸ©é˜µå•æŒ‡ç¬¬ä¸€æ­¥çš„çŸ©é˜µï¼Œå³Value downçŸ©é˜µ

![image-20250827170820421](Transformer.assets/image-20250827170820421.png)

![image-20250827170728542](Transformer.assets/image-20250827170728542.png)

#### å¤šå±‚æ„ŸçŸ¥å™¨ MLP

![image-20250827172225362](Transformer.assets/image-20250827172225362.png)

![image-20250827172255516](Transformer.assets/image-20250827172255516.png)

**æ³¨æ„åŠ›å±‚** è´Ÿè´£è·¨åºåˆ—ä½ç½®çš„ä¿¡æ¯äº¤äº’ï¼ˆè°å’Œè°ç›¸å…³ï¼‰ã€‚

**MLP å±‚** åˆ™è´Ÿè´£ **ä½ç½®å†…ç‰¹å¾çš„éçº¿æ€§å˜æ¢**ï¼Œå³é€ token çš„ç‰¹å¾æå–å’Œè¡¨è¾¾èƒ½åŠ›å¢å¼ºã€‚

- å¯ä»¥ç†è§£ä¸ºï¼šæ³¨æ„åŠ›å†³å®šâ€œä¿¡æ¯æ€ä¹ˆæµåŠ¨â€ï¼Œè€Œ MLP å†³å®šâ€œä¿¡æ¯å¦‚ä½•å˜æ¢â€ã€‚

è¿™ç§äº¤æ›¿å †å çš„ç»“æ„ä½¿ Transformer æ—¢èƒ½æ•æ‰å…¨å±€ä¾èµ–å…³ç³»ï¼Œåˆèƒ½é€ç‚¹è¿›è¡Œæ·±åº¦ç‰¹å¾æç‚¼ã€‚

# å¤ç°

## Transformeræ¶æ„

[å“ˆä½›NLPå›¢é˜Ÿå®ç°çš„Pytorchç‰ˆTransformerçš„æºç è§£æ](https://nlp.seas.harvard.edu/annotated-transformer/)

[çŸ¥ä¹ï¼šTransformeræºç è¯¦è§£ï¼ˆPytorchç‰ˆæœ¬ï¼‰](https://zhuanlan.zhihu.com/p/398039366)

### attention

```python
def attention(query, key, value, mask=None, dropout=None):
    d_k = query.size(-1)  
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)  
    if mask is not None:  
        scores = scores.masked_fill(mask == 0, -1e9)  
    p_attn = F.softmax(scores, dim=-1)  
    if dropout is not None:  
        p_attn = dropout(p_attn)  
    return torch.matmul(p_attn, value), p_attn
```

å‚æ•°ç»´åº¦ï¼ˆå¸¸è§çº¦å®šï¼‰

åœ¨ **å¤šå¤´æ³¨æ„åŠ›**çš„å®ç°é‡Œï¼Œé€šå¸¸ä¼ å…¥çš„ `query, key, value` ä¸æ˜¯åŸå§‹çš„ 3D `(batch, seq_len, d_model)`ï¼Œè€Œæ˜¯å·²ç»**æ‹†åˆ†æˆå¤šå¤´åçš„ 4D**ï¼š

- `query`: `(batch, num_heads, seq_q, d_k)`
- `key`: `(batch, num_heads, seq_k, d_k)`
- `value`: `(batch, num_heads, seq_k, d_k)`
- å…¶ä¸­ï¼š
  - `batch` = æ‰¹æ¬¡å¤§å°
  - `num_heads` = æ³¨æ„åŠ›å¤´æ•°
  - `seq_q` = query åºåˆ—é•¿åº¦
  - `seq_k` = key/value åºåˆ—é•¿åº¦ï¼ˆå¯èƒ½å’Œ `seq_q` ç›¸åŒæˆ–ä¸åŒï¼Œæ¯”å¦‚ encoder-decoder attentionï¼‰
  - `d_k` = æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç‰¹å¾ç»´åº¦ï¼ˆé€šå¸¸ `d_model / num_heads`ï¼‰

```python
d_k = query.size(-1)
```

å–å‡ºæ¯ä¸ªå¤´çš„ç»´åº¦å¤§å°ï¼ˆå³ head_dim = d_model / num_headsï¼‰ã€‚

```python
scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
```

è¿™é‡Œæ˜¯ **Scaled Dot-Product Attention** çš„æ ¸å¿ƒæ­¥éª¤ï¼š

- `key.transpose(-2, -1)` æŠŠ `key` çš„æœ€åä¸¤ä¸ªç»´åº¦äº¤æ¢ï¼Œ
   åŸå½¢çŠ¶ï¼š`(batch, num_heads, seq_k, d_k)`
   è½¬ç½®åï¼š`(batch, num_heads, d_k, seq_k)`
- `torch.matmul(query, key^T)`ï¼š
  - query: `(batch, num_heads, seq_q, d_k)`
  - key^T: `(batch, num_heads, d_k, seq_k)`
  - **ç»“æœï¼š`(batch, num_heads, seq_q, seq_k)`**
     â†’ æ¯ä¸ª query ä½ç½®å’Œæ¯ä¸ª key ä½ç½®è®¡ç®—ç›¸ä¼¼åº¦ã€‚
- é™¤ä»¥ `sqrt(d_k)`ï¼šé˜²æ­¢æ•°å€¼è¿‡å¤§ï¼Œç¨³å®šæ¢¯åº¦ã€‚

```python
if mask is not None:
    scores = scores.masked_fill(mask == 0, -1e9)
```

- `mask` çš„å½¢çŠ¶è¦èƒ½å¹¿æ’­åˆ° `(batch, num_heads, seq_q, seq_k)`
- `mask==0` çš„åœ°æ–¹è¡¨ç¤º **ä¸å¯è§**ï¼Œç”¨ `-1e9` å¡«å……ï¼Œsoftmax åæ¦‚ç‡è¶‹è¿‘äº 0ã€‚
- å¸¸è§ mask æœ‰ï¼š
  - **padding mask**: å±è”½ `<pad>` token
  - **look-ahead mask**: é˜²æ­¢ decoder çœ‹æœªæ¥è¯

```python
p_attn = F.softmax(scores, dim=-1)
```

- åœ¨æœ€åä¸€ç»´ `seq_k` ä¸Šåš softmaxã€‚
- ç»“æœ `p_attn` çš„å½¢çŠ¶ï¼š`(batch, num_heads, seq_q, seq_k)`
- è¡¨ç¤ºæ¯ä¸ª query ä½ç½®åœ¨æ‰€æœ‰ key ä¸Šçš„æ³¨æ„åŠ›åˆ†å¸ƒã€‚

```python
if dropout is not None:
    p_attn = dropout(p_attn)
```

- åœ¨è®­ç»ƒä¸­å¯¹æ³¨æ„åŠ›æƒé‡åš dropoutï¼Œå¢åŠ æ­£åˆ™åŒ–ã€‚

```python
return torch.matmul(p_attn, value), p_attn
```

- `p_attn`: `(batch, num_heads, seq_q, seq_k)`
- `value`: `(batch, num_heads, seq_k, d_k)`
- çŸ©é˜µä¹˜æ³•ç»“æœï¼š`(batch, num_heads, seq_q, d_k)`

å³æ¯ä¸ª query ä½ç½®çš„è¡¨ç¤ºæ˜¯æ‰€æœ‰ value çš„åŠ æƒå’Œã€‚

æœ€ç»ˆè¿”å›ï¼š

1. **æ³¨æ„åŠ›è¾“å‡º** `(batch, num_heads, seq_q, d_k)`
2. **æ³¨æ„åŠ›æƒé‡** `(batch, num_heads, seq_q, seq_k)`ï¼ˆå¯ç”¨äºå¯è§†åŒ–æˆ–è§£é‡Šæ¨¡å‹ï¼‰

æ€»ç»“ï¼š

- **è¾“å…¥ Q/K/Vï¼š4D `(batch, num_heads, seq_len, d_k)`**
- **scoresï¼š4D `(batch, num_heads, seq_q, seq_k)`**
- **softmax å p_attnï¼šåŒä¸Š**
- **è¾“å‡ºï¼š4D `(batch, num_heads, seq_q, d_k)`**

### clones

```python
def clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])
```

**å¤åˆ¶åŒä¸€ä¸ªæ¨¡å— N ä»½ï¼Œå‚æ•°ç‹¬ç«‹ï¼Œå¹¶ç”¨ ModuleList ç®¡ç†**ï¼Œå¸¸ç”¨äº Transformer é‡Œ **å †å  N å±‚ Encoder/Decoder**ã€‚

- **è¾“å…¥**ï¼š
  - `module`ï¼šä¸€ä¸ª `nn.Module`ï¼ˆæ¯”å¦‚ `EncoderLayer` æˆ– `SublayerConnection`ï¼‰
  - `N`ï¼šæƒ³è¦å¤åˆ¶çš„æ•°é‡
- **è¾“å‡º**ï¼š
  - `nn.ModuleList`ï¼Œé‡Œé¢æœ‰ `N` ä¸ªå®Œå…¨ç‹¬ç«‹çš„ `module` æ‹·è´

### MultiHeadedAttention

```python
class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0                             
        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value, mask=None):
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)

        # 1) Do all the linear projections in batch from d_model => h x d_k
        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]
        # 2) Apply attention on all the projected vectors in batch.
        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)

        # 3) "Concat" using a view and apply a final linear.
        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
        return self.linears[-1](x)
```

**å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰** çš„ä»£ç ã€‚

#### 1. ç±»çš„å®šä¹‰

```python
class MultiHeadedAttention(nn.Module):
```

æ¨¡å—ï¼šç”¨æ¥å®ç° **å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶**ã€‚

#### 2. æ„é€ å‡½æ•°

```python
def __init__(self, h, d_model, dropout=0.1):
    super(MultiHeadedAttention, self).__init__()
    assert d_model % h == 0
```

* `h`: å¤´æ•°ï¼ˆå¤šå°‘ä¸ªæ³¨æ„åŠ›å¤´ï¼‰ã€‚
* `d_model`: è¾“å…¥çš„æ€»ç»´åº¦ã€‚
* `assert d_model % h == 0`: ä¿è¯å¯ä»¥å‡åŒ€åˆ†æˆ h ä¸ªå¤´ã€‚

```python
    self.d_k = d_model // h
    self.h = h
```

* `self.d_k`: æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦å¤§å°ã€‚
* `self.h`: å¤´æ•°ã€‚

```python
    self.linears = clones(nn.Linear(d_model, d_model), 4)
```

è¿™é‡Œç”Ÿæˆäº† **4 ä¸ªçº¿æ€§å±‚**ï¼š

1. ç»™ **query** åšæŠ•å½±
2. ç»™ **key** åšæŠ•å½±
3. ç»™ **value** åšæŠ•å½±
4. æœ€åæ‹¼æ¥åå†åšä¸€æ¬¡çº¿æ€§å˜æ¢

 `self.linears` æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ˆé•¿åº¦ä¸º 4ï¼‰ï¼Œé‡Œé¢æœ‰ 4 ä¸ªç‹¬ç«‹çš„ `nn.Linear(d_model, d_model)` å±‚ã€‚

```python
    self.attn = None
    self.dropout = nn.Dropout(p=dropout)
```

* `self.attn`: ä¿å­˜æ³¨æ„åŠ›æƒé‡ï¼ˆå¯è§†åŒ–æˆ–è°ƒè¯•ç”¨ï¼‰ã€‚
* `self.dropout`: åœ¨æ³¨æ„åŠ›åˆ†æ•°ä¸ŠåŠ  dropoutï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚

---

#### 3. å‰å‘ä¼ æ’­

```python
def forward(self, query, key, value, mask=None):
```

è¾“å…¥å‚æ•°ï¼š

* `query, key, value`: è¾“å…¥å¼ é‡ï¼ˆé€šå¸¸ shape = `[batch_size, seq_len, d_model]`ï¼‰ã€‚
* `mask`: æ©ç ï¼Œç”¨äºå±è”½æŸäº›ä½ç½®ï¼ˆæ¯”å¦‚ Transformer çš„è§£ç å™¨ç”¨æ¥é˜²æ­¢çœ‹æœªæ¥çš„ä¿¡æ¯ï¼‰ã€‚

```python
    if mask is not None:
        mask = mask.unsqueeze(1)
```

æ‰©å±• `mask` ç»´åº¦ï¼Œä½¿å…¶å¯ä»¥åº”ç”¨åˆ°æ‰€æœ‰å¤´ã€‚

```python
    nbatches = query.size(0)
```

è·å– batch sizeã€‚

---

#### 4. æŠ•å½±åˆ°å¤šå¤´ç©ºé—´

```python
    query, key, value = [
        l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
        for l, x in zip(self.linears, (query, key, value))
    ]
```

è¿™ä¸€è¡Œæ˜¯æ ¸å¿ƒï¼š

è¿™é‡Œé‡‡ç”¨äº† **åˆ—è¡¨æ¨å¯¼å¼** å†™æ³•ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ª **for å¾ªç¯** çš„ç®€å†™ï¼Œåªæ˜¯å†™åœ¨ä¸€è¡Œé‡Œã€‚

**æµç¨‹ï¼šåŸå§‹è¾“å…¥ (d_model ç»´) â†’ çº¿æ€§å±‚æŠ•å½± (è¿˜æ˜¯ d_model ç»´) â†’ reshape åˆ†æˆ h ä¸ªå¤´ (d_k ç»´) â†’ è½¬ç½®æ–¹ä¾¿è®¡ç®—ã€‚**

1. `l(x)`ï¼šå…ˆé€šè¿‡å¯¹åº”çš„çº¿æ€§å±‚æŠ•å½±ï¼Œç»´åº¦è¿˜æ˜¯ `(batch, seq_len, d_model)`ã€‚
2. `.view(nbatches, -1, self.h, self.d_k)`ï¼šæ‹†åˆ†æˆå¤šå¤´ `(batch, seq_len, h, d_k)`ã€‚
3. `.transpose(1, 2)`ï¼šè°ƒæ•´ç»´åº¦é¡ºåº â†’ `(batch, h, seq_len, d_k)`ï¼Œæ–¹ä¾¿åé¢è®¡ç®—æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›ã€‚
4. `zip(self.linears, (query, key, value))`ï¼š`zip` ä¼šæŠŠä¸¤ä¸ªå¯è¿­ä»£å¯¹è±¡æŒ‰é¡ºåºé…å¯¹ã€‚
    è¿™é‡Œåªå–å‰ 3 ä¸ªçº¿æ€§å±‚å»å¯¹åº” 3 ä¸ªè¾“å…¥
5. `for l, x in zip(...)`ï¼š
6. å°±æ˜¯æŠŠè¿™ä¸‰å¯¹åˆ†åˆ«å±•å¼€ï¼š

   - ç¬¬ä¸€æ¬¡å¾ªç¯ï¼š`l = Linear1, x = query`

   - ç¬¬äºŒæ¬¡å¾ªç¯ï¼š`l = Linear2, x = key`

   - ç¬¬ä¸‰æ¬¡å¾ªç¯ï¼š`l = Linear3, x = value`

---

#### 5. è®¡ç®—æ³¨æ„åŠ›

```python
    x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)
```

è°ƒç”¨ `attention` å‡½æ•°ï¼ˆé€šå¸¸æ˜¯ **ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›**ï¼‰ï¼š

$$
\text{Attention}(Q,K,V) = \text{softmax}\Big(\frac{QK^T}{\sqrt{d_k}} + \text{mask}\Big)V
$$

è¾“å‡ºï¼š

* `x`: å¤šå¤´æ³¨æ„åŠ›çš„ç»“æœ `(batch, h, seq_len, d_k)`ã€‚
* `self.attn`: æ³¨æ„åŠ›æƒé‡ `(batch, h, seq_len, seq_len)`ã€‚

---

#### 6. æ‹¼æ¥ + æœ€ç»ˆçº¿æ€§å˜æ¢

```python
    x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
```

* `.transpose(1, 2)`ï¼šå…ˆæŠŠç»´åº¦è½¬å›æ¥ `(batch, seq_len, h, d_k)`ã€‚
* `.contiguous()`ï¼šåœ¨å†…å­˜é‡Œå¤åˆ¶ä¸€ä»½æ•°æ®ï¼Œä½¿å¼ é‡çš„å­˜å‚¨å˜æˆè¿ç»­çš„ã€‚è¿™æ ·æ‰èƒ½å®‰å…¨åœ°è°ƒç”¨ `.view()`ã€‚
* `.view` å±•å¹³æœ€åä¸¤ä¸ªç»´åº¦ï¼Œå¾—åˆ° `(batch, seq_len, d_model)`ã€‚

```python
    return self.linears[-1](x)
```

æœ€åç”¨ç¬¬ 4 ä¸ªçº¿æ€§å±‚åšä¸€æ¬¡æ˜ å°„ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚

---

#### æ€»ç»“

è¿™ä¸ªæ¨¡å—åšçš„äº‹å°±æ˜¯æ ‡å‡†çš„ **Multi-Head Attention**ï¼š

1. ç”¨ 3 ä¸ªçº¿æ€§å±‚åˆ†åˆ«æŠ•å½± Qã€Kã€V
2. æ‹†åˆ†æˆå¤šä¸ªå¤´ `(h)`
3. åœ¨æ¯ä¸ªå¤´é‡Œåšç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
4. æŠŠæ‰€æœ‰å¤´æ‹¼æ¥èµ·æ¥
5. å†ç”¨ä¸€ä¸ªçº¿æ€§å±‚èåˆ

### PositionwiseFeedForward

```python
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))
```

**Positionwise Feedforward** å±‚ï¼Œé€šå¸¸åœ¨ **Transformer** æ¨¡å‹ä¸­çš„æ¯ä¸ªç¼–ç å™¨å’Œè§£ç å™¨å±‚ä¸­ä½œä¸ºåç»­å¤„ç†å±‚ã€‚å®ƒå¯¹è¾“å…¥çš„æ¯ä¸ªä½ç½®éƒ½åº”ç”¨ç›¸åŒçš„å‰é¦ˆç½‘ç»œæ“ä½œã€‚

#### 1. ç±»çš„å®šä¹‰

```python
class PositionwiseFeedForward(nn.Module):
```

`PositionwiseFeedForward` ç±»ç»§æ‰¿äº† `nn.Module`ï¼Œè¡¨ç¤ºä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å—ï¼Œç”¨äºå®ç°å‰é¦ˆç¥ç»ç½‘ç»œå±‚ã€‚

#### 2. æ„é€ å‡½æ•°

```python
def __init__(self, d_model, d_ff, dropout=0.1):
    super(PositionwiseFeedForward, self).__init__()
```

- `d_model`: è¾“å…¥çš„ç»´åº¦å¤§å°ã€‚é€šå¸¸æ˜¯ **Transformer** ä¸­çš„ `d_model`ï¼Œä¾‹å¦‚ 512 æˆ– 1024ã€‚
- `d_ff`: å‰é¦ˆç½‘ç»œçš„éšå±‚ç»´åº¦ï¼Œé€šå¸¸æ¯” `d_model` å¤§ï¼Œè¡¨ç¤ºç½‘ç»œçš„æ‰©å±•ç»´åº¦ã€‚
- `dropout`: åœ¨å‰é¦ˆç½‘ç»œçš„æ¿€æ´»å€¼ä¸Šåº”ç”¨ dropoutï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé»˜è®¤å€¼æ˜¯ 0.1ã€‚

```python
    self.w_1 = nn.Linear(d_model, d_ff)
    self.w_2 = nn.Linear(d_ff, d_model)
    self.dropout = nn.Dropout(dropout)
```

- `self.w_1`: ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå°†è¾“å…¥ç»´åº¦ `d_model` æ˜ å°„åˆ°éšå±‚ç»´åº¦ `d_ff`ã€‚
- `self.w_2`: å¦ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå°†éšå±‚çš„ `d_ff` æ˜ å°„å›åŸå§‹çš„ `d_model` ç»´åº¦ã€‚
- `self.dropout`: dropout å±‚ï¼Œç”¨äºåœ¨å‰é¦ˆç½‘ç»œçš„æ¿€æ´»å€¼ä¸Šè¿›è¡Œæ­£åˆ™åŒ–ã€‚

#### 3. å‰å‘ä¼ æ’­

```python
def forward(self, x):
    return self.w_2(self.dropout(F.relu(self.w_1(x))))
```

- `x` æ˜¯è¾“å…¥å¼ é‡ï¼Œé€šå¸¸å½¢çŠ¶ä¸º `[batch_size, seq_len, d_model]`ï¼Œå³æ¯ä¸ªä½ç½®çš„ç‰¹å¾æ˜¯ `d_model` ç»´åº¦çš„ã€‚

**å‰å‘ä¼ æ’­çš„æ“ä½œï¼š**

1. `self.w_1(x)`ï¼šå…ˆé€šè¿‡ç¬¬ä¸€ä¸ªçº¿æ€§å±‚ `w_1`ï¼Œå°†è¾“å…¥çš„ `d_model` ç»´åº¦æ˜ å°„åˆ° `d_ff` ç»´åº¦ã€‚
2. `F.relu(...)`ï¼šå¯¹è¾“å‡ºåº”ç”¨ ReLU æ¿€æ´»å‡½æ•°ã€‚
3. `self.dropout(...)`ï¼šå¯¹ ReLU æ¿€æ´»åçš„ç»“æœåº”ç”¨ dropoutã€‚
4. `self.w_2(...)`ï¼šæœ€åé€šè¿‡ç¬¬äºŒä¸ªçº¿æ€§å±‚ `w_2`ï¼Œå°† `d_ff` ç»´åº¦çš„ç‰¹å¾æ˜ å°„å›åŸå§‹çš„ `d_model` ç»´åº¦ã€‚

#### æ€»ç»“

è¿™ä¸ªå±‚çš„ä½œç”¨æ˜¯ï¼š

1. å¯¹è¾“å…¥è¿›è¡Œä¸¤æ¬¡çº¿æ€§æ˜ å°„ï¼ˆä¸€ä¸ªæ‰©å±•ç»´åº¦ï¼Œå¦ä¸€ä¸ªæ¢å¤ç»´åº¦ï¼‰ã€‚
2. åœ¨ä¸¤æ¬¡æ˜ å°„ä¹‹é—´åº”ç”¨ ReLU æ¿€æ´»å‡½æ•°å’Œ dropoutã€‚
3. å®ƒæ˜¯ä½ç½®æ— å…³çš„ï¼Œå³å¯¹æ¯ä¸ªä½ç½®çš„è¾“å…¥éƒ½åº”ç”¨ç›¸åŒçš„å‰é¦ˆç½‘ç»œã€‚

### Embeddings

```python
class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model

    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)
```

**Embedding** å±‚ï¼Œç”¨äºå°†ç¦»æ•£çš„è¯æ±‡ç´¢å¼•ï¼ˆå¦‚è¯è¡¨ä¸­çš„å•è¯ï¼‰æ˜ å°„åˆ°å¯†é›†çš„ **è¯å‘é‡**ï¼ˆå³ä¸€ä¸ªå›ºå®šç»´åº¦çš„å®æ•°å‘é‡ï¼‰ï¼Œå¹¶å¯¹åµŒå…¥å‘é‡åšä¸€ä¸ªç¼©æ”¾æ“ä½œï¼Œå¢å¼ºæ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚ç”¨æ¥è¡¨ç¤ºè¾“å…¥çš„è¯æ±‡ã€‚

#### 1. ç±»çš„å®šä¹‰

```python
class Embeddings(nn.Module):
```

`Embeddings` ç±»ç»§æ‰¿è‡ª `nn.Module`ï¼Œè¡¨ç¤ºä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å—ï¼Œç”¨äºç”Ÿæˆè¯åµŒå…¥ã€‚

#### 2. æ„é€ å‡½æ•°

```python
def __init__(self, d_model, vocab):
    super(Embeddings, self).__init__()
    self.lut = nn.Embedding(vocab, d_model)
    self.d_model = d_model
```

* `d_model`: æ¯ä¸ªè¯æ±‡çš„åµŒå…¥ç»´åº¦ï¼Œ**Transformer** æ¨¡å‹çš„ç‰¹å¾ç»´åº¦ï¼ˆå¦‚ 512 æˆ– 1024ï¼‰ã€‚
* `vocab`: è¯æ±‡è¡¨çš„å¤§å°ï¼Œè¡¨ç¤ºè¯è¡¨ä¸­ä¸åŒçš„è¯æ±‡æ•°é‡ã€‚

**`self.lut = nn.Embedding(vocab, d_model)`**ï¼š

* é€šè¿‡ `nn.Embedding` åˆ›å»ºäº†ä¸€ä¸ª**è¯åµŒå…¥å±‚**ï¼Œå®ƒçš„ä½œç”¨æ˜¯å°†è¾“å…¥çš„è¯æ±‡ç´¢å¼•ï¼ˆæ•´æ•°ï¼‰æ˜ å°„åˆ°ä¸€ä¸ªè¿ç»­ç©ºé—´çš„å‘é‡ï¼ˆç»´åº¦ä¸º `d_model`ï¼‰ã€‚
* è¯¥å±‚ä¼šå­¦ä¹ ä¸€ä¸ªå¤§å°ä¸º `[vocab, d_model]` çš„æƒé‡çŸ©é˜µï¼Œ`vocab` è¡¨ç¤ºè¯æ±‡è¡¨ä¸­çš„è¯æ±‡æ•°ï¼Œ`d_model` è¡¨ç¤ºæ¯ä¸ªè¯çš„åµŒå…¥ç»´åº¦ã€‚

```python
    self.d_model = d_model
```

* è®°å½• `d_model` çš„å€¼ã€‚

#### 3. å‰å‘ä¼ æ’­

```python
def forward(self, x):
    return self.lut(x) * math.sqrt(self.d_model)
```

* `x` æ˜¯è¾“å…¥çš„ç´¢å¼•å¼ é‡ï¼Œé€šå¸¸å½¢çŠ¶ä¸º `[batch_size, seq_len]`ï¼Œå…¶ä¸­ `seq_len` æ˜¯åºåˆ—çš„é•¿åº¦ã€‚æ¯ä¸ªå…ƒç´ éƒ½æ˜¯è¯æ±‡è¡¨ä¸­è¯æ±‡çš„ç´¢å¼•ã€‚

**å‰å‘ä¼ æ’­çš„æ“ä½œï¼š**

1. **`self.lut(x)`**ï¼šé€šè¿‡ `nn.Embedding` å±‚å°†è¾“å…¥çš„ç´¢å¼• `x` **æ˜ å°„ä¸ºåµŒå…¥å‘é‡**ï¼Œå¾—åˆ°ä¸€ä¸ªå½¢çŠ¶ä¸º `[batch_size, seq_len, d_model]` çš„è¾“å‡ºå¼ é‡ã€‚
2. **`* math.sqrt(self.d_model)`**ï¼š**å°†åµŒå…¥å‘é‡ç¼©æ”¾**ã€‚æ ¹æ® **Transformer** ä¸­çš„æƒ¯ä¾‹ï¼Œè¯åµŒå…¥çš„ç»´åº¦éœ€è¦ä¹˜ä»¥ä¸€ä¸ªå¸¸æ•°å› å­ï¼ˆé€šå¸¸æ˜¯ `sqrt(d_model)`ï¼‰ï¼Œè¿™**æœ‰åŠ©äºé˜²æ­¢åµŒå…¥å‘é‡çš„åˆå§‹å€¼è¿‡å¤§æˆ–è€…è¿‡å°ï¼Œä»è€Œæœ‰åˆ©äºæ¢¯åº¦çš„ç¨³å®šä¼ æ’­ã€‚**

#### æ€»ç»“

è¯¥æ¨¡å—å®ç°äº† **è¯åµŒå…¥** çš„åŠŸèƒ½ï¼š

1. ä½¿ç”¨ `nn.Embedding` å°†æ¯ä¸ªè¯æ±‡ç´¢å¼•æ˜ å°„ä¸ºä¸€ä¸ªå¯†é›†çš„å‘é‡è¡¨ç¤ºã€‚
2. å°†åµŒå…¥å‘é‡ä¹˜ä»¥ `sqrt(d_model)`ï¼Œé¿å…åˆå§‹åŒ–æ—¶å‘é‡è¿‡å°ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒã€‚

è¿™ç§æ–¹å¼èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†ç¦»æ•£çš„è¯æ±‡ä¿¡æ¯è½¬åŒ–ä¸ºè¿ç»­çš„å‘é‡è¡¨ç¤ºï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å­¦ä¹ è¿™äº›è¯å‘é‡æ¥ç†è§£è¯­ä¹‰ä¿¡æ¯ã€‚

### PositionalEncoding

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) *
                             -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)
        return self.dropout(x)
```

è¿™æ®µä»£ç å®ç°äº† **Transformer é‡Œçš„ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰**ï¼Œå®ƒçš„ä½œç”¨æ˜¯ä¸ºè¯å‘é‡åŠ å…¥ä½ç½®ä¿¡æ¯

#### 1. ç±»å®šä¹‰

```python
class PositionalEncoding(nn.Module):
```

ç»§æ‰¿è‡ª `nn.Module`ï¼Œç”¨äºç”Ÿæˆ **ä½ç½®ç¼–ç ** å¹¶åŠ åˆ°è¾“å…¥çš„è¯å‘é‡ä¸Šã€‚

#### 2. æ„é€ å‡½æ•°

```python
def __init__(self, d_model, dropout, max_len=5000):
    super(PositionalEncoding, self).__init__()
    self.dropout = nn.Dropout(p=dropout)
```

* `d_model`: è¯å‘é‡çš„ç»´åº¦ï¼ˆä¾‹å¦‚ 512ï¼‰ã€‚
* `dropout`: **åœ¨ä½ç½®ç¼–ç åŠ åˆ°è¾“å…¥åï¼Œå†åš dropout**ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚
* `max_len`: å…è®¸çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆä¾‹å¦‚ 5000ï¼‰ã€‚

```python
    pe = torch.zeros(max_len, d_model)   # (max_len, d_model)ï¼Œä¿å­˜æ‰€æœ‰ä½ç½®çš„ç¼–ç 
    position = torch.arange(0, max_len).unsqueeze(1)  # shape = (max_len, 1)ï¼Œè¡¨ç¤ºæ¯ä¸ªä½ç½®çš„ç´¢å¼•
```

* `pe` ç”¨æ¥å­˜å‚¨ä½ç½®ç¼–ç çŸ©é˜µã€‚
* `position` å°±æ˜¯ `[0,1,2,...,max_len-1]` çš„ä½ç½®ç´¢å¼•ã€‚

```python
    div_term = torch.exp(torch.arange(0, d_model, 2) *
                         -(math.log(10000.0) / d_model))
```

è¿™é‡Œæ˜¯ç¼©æ”¾å› å­ï¼š

$$
\text{div\_term}[i] = 10000^{- \frac{2i}{d_{model}}}
$$

ç”¨äºä¸åŒç»´åº¦çš„é¢‘ç‡ç¼©æ”¾ï¼Œè®©æ­£å¼¦/ä½™å¼¦å‡½æ•°èƒ½è¦†ç›–ä¸åŒçš„å‘¨æœŸèŒƒå›´ã€‚

#### 3. ç”Ÿæˆä½ç½®ç¼–ç 

```python
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
```

* `0::2` â†’ å¶æ•°ç»´åº¦ç”¨ `sin`
* `1::2` â†’ å¥‡æ•°ç»´åº¦ç”¨ `cos`

è¿™æ ·å°±å¾—åˆ°äº†ï¼š

$$
PE_{(pos, 2i)} = \sin\Big(\frac{pos}{10000^{2i/d_{model}}}\Big)
$$

$$
PE_{(pos, 2i+1)} = \cos\Big(\frac{pos}{10000^{2i/d_{model}}}\Big)
$$

è¿™æ ·è®¾è®¡çš„å¥½å¤„æ˜¯ï¼š**ä»»æ„ä½ç½®ä¹‹é—´çš„ä½ç½®ç¼–ç å¯ä»¥çº¿æ€§è¡¨ç¤ºç›¸å¯¹ä½ç½®å…³ç³»**ã€‚

#### 4. è°ƒæ•´ç»´åº¦å¹¶æ³¨å†Œä¸ºç¼“å†²åŒº

```python
    pe = pe.unsqueeze(0)   # shape: (1, max_len, d_model)
    self.register_buffer('pe', pe)
```

* `.unsqueeze(0)` ï¼šåœ¨ç¬¬ 0 ç»´å¢åŠ ç»´åº¦ï¼Œå¢åŠ  batch ç»´åº¦ï¼Œæ–¹ä¾¿å’Œè¾“å…¥ç›¸åŠ ã€‚
* `register_buffer` è¡¨ç¤ºè¿™æ˜¯**æ¨¡å‹çš„æŒä¹…çŠ¶æ€**ï¼ŒæŠŠ `pe` ä¿å­˜åˆ°æ¨¡å‹é‡Œï¼Œä½†**ä¸ä½œä¸ºå¯è®­ç»ƒå‚æ•°ã€‚**
  * **ä½ç½®ç¼–ç æ˜¯å›ºå®šçš„æ­£å¼¦/ä½™å¼¦å‡½æ•°ï¼Œä¸éœ€è¦å­¦ä¹ **ã€‚


#### 5. å‰å‘ä¼ æ’­

```python
def forward(self, x):
    x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)
    return self.dropout(x)
```

* `x`: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ `(batch_size, seq_len, d_model)`ã€‚
* `self.pe[:, :x.size(1)]`: å–å‰ `seq_len` ä¸ªä½ç½®çš„ç¼–ç ï¼ˆå› ä¸ºè¾“å…¥åºåˆ—å¯èƒ½æ¯” `max_len` çŸ­ï¼‰ã€‚
* `x + position_encoding`: å°†ä½ç½®ç¼–ç åŠ åˆ°è¯åµŒå…¥ä¸Šã€‚
* æœ€åé€šè¿‡ `dropout` è¿”å›ã€‚

#### æ€»ç»“

è¿™ä¸ªæ¨¡å—çš„ä½œç”¨æ˜¯ï¼š

1. **æ„é€ ä¸€ä¸ªå›ºå®šçš„æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç çŸ©é˜µ**ï¼Œè¦†ç›– `[0, max_len]` çš„æ‰€æœ‰ä½ç½®ã€‚
2. **åœ¨å‰å‘ä¼ æ’­æ—¶**ï¼Œæ ¹æ®è¾“å…¥åºåˆ—çš„é•¿åº¦ï¼Œå–å¯¹åº”é•¿åº¦çš„ä½ç½®ç¼–ç ï¼ŒåŠ åˆ°è¾“å…¥çš„è¯å‘é‡ä¸Šã€‚
3. é€šè¿‡ **sin å’Œ cos çš„ä¸åŒå‘¨æœŸ**ï¼Œæ¨¡å‹å¯ä»¥æ„ŸçŸ¥åºåˆ—ä¸­çš„ç»å¯¹å’Œç›¸å¯¹ä½ç½®ã€‚

### Layer Normalization

```python
class LayerNorm(nn.Module):
    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2
```

**Layer Normalizationï¼ˆå±‚å½’ä¸€åŒ–ï¼‰**

å’Œ BatchNorm ä¸åŒï¼ŒLayerNorm æ˜¯åœ¨ **ç‰¹å¾ç»´åº¦** ä¸Šåšå½’ä¸€åŒ–çš„ï¼ˆå³æ¯ä¸ªæ ·æœ¬è‡ªå·±å½’ä¸€åŒ–ï¼Œä¸ä¾èµ– batch é‡Œçš„å…¶ä»–æ ·æœ¬ï¼‰ï¼Œ**å°±æ˜¯å¯¹ä¸€ä¸ª token çš„åµŒå…¥å‘é‡ï¼ˆå³æœ€åä¸€ç»´ `d_model`ï¼‰åšå½’ä¸€åŒ–**ã€‚

#### 1. ç±»çš„å®šä¹‰

```python
class LayerNorm(nn.Module):
```

ç»§æ‰¿è‡ª `nn.Module`ï¼Œç”¨äºå®ç° Layer Normalizationã€‚

#### 2. æ„é€ å‡½æ•°

```python
def __init__(self, features, eps=1e-6):
    super(LayerNorm, self).__init__()
    self.a_2 = nn.Parameter(torch.ones(features))
    self.b_2 = nn.Parameter(torch.zeros(features))
    self.eps = eps
```

* `features`: ç‰¹å¾çš„ç»´åº¦å¤§å°ï¼ˆä¾‹å¦‚è¾“å…¥ `x` çš„æœ€åä¸€ç»´é•¿åº¦ï¼‰ã€‚
* `eps`: ä¸€ä¸ªå¾ˆå°çš„æ•°ï¼Œé˜²æ­¢é™¤é›¶é”™è¯¯ï¼Œé»˜è®¤ `1e-6`ã€‚

ä¸¤ä¸ªå¯å­¦ä¹ å‚æ•°ï¼š

* `self.a_2`ï¼ˆç›¸å½“äº Î³ï¼‰ï¼šç¼©æ”¾å‚æ•°ï¼Œåˆå§‹ä¸º 1ã€‚
* `self.b_2`ï¼ˆç›¸å½“äº Î²ï¼‰ï¼šå¹³ç§»å‚æ•°ï¼Œåˆå§‹ä¸º 0ã€‚

**è¿™ä¸¤ä¸ªå‚æ•°ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ ï¼Œä¿è¯å½’ä¸€åŒ–åè¿˜èƒ½æ¢å¤æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚**

#### 3. å‰å‘ä¼ æ’­

```python
def forward(self, x):
    mean = x.mean(-1, keepdim=True)
    std = x.std(-1, keepdim=True)
    return self.a_2 * (x - mean) / (std + self.eps) + self.b_2
```

**å…³é”®æ­¥éª¤ï¼š**

1. **è®¡ç®—å‡å€¼**

   ```python
   mean = x.mean(-1, keepdim=True)
   ```

   * åœ¨æœ€åä¸€ç»´ï¼ˆç‰¹å¾ç»´åº¦ï¼‰ä¸Šæ±‚å‡å€¼ã€‚
   * `keepdim=True` ä¿æŒç»´åº¦æ–¹ä¾¿åç»­å¹¿æ’­ã€‚

2. **è®¡ç®—æ ‡å‡†å·®**

   ```python
   std = x.std(-1, keepdim=True)
   ```

   * åœ¨æœ€åä¸€ç»´ä¸Šæ±‚æ ‡å‡†å·®ã€‚

3. **å½’ä¸€åŒ–**

   ```python
   (x - mean) / (std + self.eps)
   ```

   * å‡å»å‡å€¼ï¼Œå†é™¤ä»¥æ ‡å‡†å·®ï¼Œå®ç°æ ‡å‡†åŒ–ã€‚
   * `eps` é˜²æ­¢é™¤é›¶é”™è¯¯ã€‚

4. **ç¼©æ”¾å’Œå¹³ç§»**

   ```python
   self.a_2 * (...) + self.b_2
   ```

   * Î³ï¼ˆ`a_2`ï¼‰æ§åˆ¶ç¼©æ”¾ï¼ŒÎ²ï¼ˆ`b_2`ï¼‰æ§åˆ¶å¹³ç§»ã€‚
   * è¿™æ ·æ¨¡å‹åœ¨å½’ä¸€åŒ–çš„åŒæ—¶ï¼Œä¾ç„¶èƒ½å­¦ä¹ åˆ°åˆé€‚çš„åˆ†å¸ƒã€‚

####  **Î³ï¼ˆa\_2ï¼‰** å’Œ **Î²ï¼ˆb\_2ï¼‰** åœ¨ LayerNorm é‡Œçš„ä½œç”¨

- LayerNorm å…ˆå¯¹è¾“å…¥åšå½’ä¸€åŒ–ï¼š


$$
\hat{x} = \frac{x - \mu}{\sigma + \epsilon}
$$

- è¿™æ ·åšçš„ç»“æœæ˜¯ï¼š

  * å½’ä¸€åŒ–åçš„å‡å€¼ = 0

  * å½’ä¸€åŒ–åçš„æ–¹å·® = 1


- ä½†æ˜¯ï¼å¦‚æœåªæ˜¯è¿™æ ·ï¼Œ**æ¨¡å‹ä¼šå¤±å»è¡¨è¾¾èƒ½åŠ›**ï¼šæ‰€æœ‰ç‰¹å¾çš„åˆ†å¸ƒéƒ½è¢«å›ºå®šåœ¨æ ‡å‡†æ­£æ€åˆ†å¸ƒäº†ã€‚

  - ä¸ºäº†è®©æ¨¡å‹ä¿ç•™è¡¨è¾¾èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥ä¸¤ä¸ªå¯å­¦ä¹ å‚æ•°ï¼š


$$
y = \gamma \cdot \hat{x} + \beta
$$

- `self.a_2` ï¼ˆÎ³ï¼‰ï¼šç¼©æ”¾å‚æ•°ï¼Œåˆå§‹åŒ–ä¸º 1
- `self.b_2` ï¼ˆÎ²ï¼‰ï¼šå¹³ç§»å‚æ•°ï¼Œåˆå§‹åŒ–ä¸º 0

- Î³ï¼ˆa\_2ï¼‰å’Œ Î²ï¼ˆb\_2ï¼‰çš„ä½œç”¨

  * **Î³ï¼ˆa\_2ï¼Œç¼©æ”¾ï¼‰â†’ æ§åˆ¶ç‰¹å¾å¤§å°**
    * æ§åˆ¶æ¯ä¸ªç‰¹å¾å½’ä¸€åŒ–åçš„å¹…åº¦ã€‚
    * å¦‚æœ Î³ å­¦å¾—å¾ˆå¤§ï¼Œå°±æŠŠè¯¥ç‰¹å¾â€œæ”¾å¤§â€ï¼›å­¦å¾—å¾ˆå°ï¼Œå°±â€œå‹ç¼©â€ã€‚
    * è®©æ¨¡å‹èƒ½è‡ªé€‚åº”ä¸åŒç‰¹å¾çš„é‡è¦æ€§ã€‚
    
  * **Î²ï¼ˆb\_2ï¼Œå¹³ç§»ï¼‰â†’ æ§åˆ¶ç‰¹å¾åç§»**
    * æ§åˆ¶å½’ä¸€åŒ–åçš„åç§»é‡ã€‚
    * å¦‚æœå½’ä¸€åŒ–ç»“æœéƒ½å›´ç»• 0ï¼Œè€Œæ¨¡å‹æ›´å¸Œæœ›ç‰¹å¾å›´ç»• 5ï¼Œé‚£ Î² å°±ä¼šå­¦åˆ°ä¸€ä¸ªåç§»å€¼ã€‚


* å®ƒä»¬ä¿è¯äº† **å½’ä¸€åŒ–ä¸ä¼šæŸå®³æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›**ï¼Œè€Œæ˜¯ç»™æ¨¡å‹ä¸€ä¸ª**â€œé‡æ–°è°ƒæ•´åˆ†å¸ƒâ€çš„è‡ªç”±åº¦**ã€‚

#### æ€»ç»“

è¿™æ®µä»£ç çš„ä½œç”¨å°±æ˜¯ï¼š

* **å¯¹æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´åº¦åšæ ‡å‡†åŒ–**ï¼ˆå³ LayerNormï¼‰ã€‚
* ä¿è¯ä¸åŒä½ç½®çš„ç‰¹å¾åœ¨è®­ç»ƒä¸­ä¸ä¼šå› ä¸ºæ•°å€¼è¿‡å¤§/è¿‡å°è€Œå½±å“è®­ç»ƒç¨³å®šæ€§ã€‚
* æä¾›å¯å­¦ä¹ çš„ç¼©æ”¾å’Œå¹³ç§»å‚æ•°ï¼Œå¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€‚

å…¬å¼å¯¹åº”ä¸ºï¼š

$$
\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sigma + \epsilon} + \beta
$$

å…¶ä¸­ï¼š

* $\mu$ï¼šæœ€åä¸€ç»´çš„å‡å€¼
* $\sigma$ï¼šæœ€åä¸€ç»´çš„æ ‡å‡†å·®
* $\gamma = \text{a\_2}, \ \beta = \text{b\_2}$

### SublayerConnection

```python
class SublayerConnection(nn.Module):

    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm(x)))
```

è¿™æ®µä»£ç å®ç°äº† **Transformer ä¸­çš„å­å±‚è¿æ¥ï¼ˆSublayer Connectionï¼‰**

- **æ®‹å·®è¿æ¥ + LayerNorm + Dropout** çš„ç»„åˆ

#### 1. ç±»å®šä¹‰

```python
class SublayerConnection(nn.Module):
```

ç»§æ‰¿è‡ª `nn.Module`ï¼Œç”¨äºæŠŠ **æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ– + Dropout** å°è£…æˆä¸€ä¸ªæ¨¡å—ã€‚

#### 2. æ„é€ å‡½æ•°

```python
def __init__(self, size, dropout):
    super(SublayerConnection, self).__init__()
    self.norm = LayerNorm(size)
    self.dropout = nn.Dropout(dropout)
```

* `size`ï¼šè¾“å…¥ç‰¹å¾çš„ç»´åº¦å¤§å°ï¼ˆé€šå¸¸å°±æ˜¯ `d_model`ï¼‰ã€‚
* `dropout`ï¼šåœ¨å­å±‚è¾“å‡ºä¸Šåš dropout çš„æ¦‚ç‡ã€‚

**åˆå§‹åŒ–äº†ä¸¤ä¸ªç»„ä»¶**ï¼š

1. `LayerNorm(size)`ï¼šå¯¹è¾“å…¥åšå±‚å½’ä¸€åŒ–ï¼Œä¿è¯æ¯ä¸ªæ ·æœ¬ç‰¹å¾ç»´åº¦ç¨³å®šã€‚
2. `Dropout(dropout)`ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

#### 3. å‰å‘ä¼ æ’­

```python
def forward(self, x, sublayer):
    return x + self.dropout(sublayer(self.norm(x)))
```

**æ­¥éª¤æ‹†è§£ï¼š**

1. **å…ˆå½’ä¸€åŒ–è¾“å…¥**

```python
self.norm(x)
```

* å¯¹è¾“å…¥ `x` åš LayerNormï¼Œå¾—åˆ°ç¨³å®šçš„ç‰¹å¾åˆ†å¸ƒã€‚

2. **è°ƒç”¨å­å±‚å‡½æ•°**

```python
sublayer(self.norm(x))
```

* **`sublayer` æ˜¯ä¸€ä¸ªå…·ä½“æ“ä½œçš„å‡½æ•°ï¼ˆæ¯”å¦‚ `Multi-Head Attention` æˆ– `PositionwiseFeedForward`ï¼‰**ï¼Œä½œç”¨äºå½’ä¸€åŒ–åçš„è¾“å…¥ã€‚
* è¿™é‡ŒæŠŠå­å±‚æ“ä½œ **å°è£…æˆå¯ä¼ å…¥çš„å‡½æ•°**ï¼Œçµæ´»æ€§å¾ˆé«˜ã€‚

3. **åŠ  Dropout**

```python
self.dropout(...)
```

* å¯¹å­å±‚è¾“å‡ºåŠ  Dropoutï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

4. **æ®‹å·®è¿æ¥**

```python
x + ...
```

* æœ€åæŠŠå­å±‚çš„è¾“å‡ºåŠ å›åŸå§‹è¾“å…¥ `x`ï¼ˆresidual connectionï¼‰ã€‚

#### 4. æ€»ç»“

`SublayerConnection` å°±æ˜¯ Transformer ä¸­æ¯ä¸ªå­å±‚çš„æ ‡å‡†å¥—è·¯ï¼š
$$
\text{Output} = x + \text{Dropout}(\text{Sublayer}(\text{LayerNorm}(x)))
$$

* `LayerNorm`ï¼šç¨³å®šè®­ç»ƒ
* `Sublayer`ï¼šå…·ä½“æ“ä½œï¼ˆæ³¨æ„åŠ›æˆ–å‰é¦ˆç½‘ç»œï¼‰
* `Dropout`ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
* `Residual`ï¼šä¿è¯æ¢¯åº¦é¡ºåˆ©å›ä¼ 

### EncoderLayer

```python
class EncoderLayer(nn.Module):
    def __init__(self, size, self_attn, feed_forward, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 2)
        self.size = size

    def forward(self, x, mask):
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
        return self.sublayer[1](x, self.feed_forward)
```

è¿™æ®µä»£ç å®ç°äº† **Transformer ç¼–ç å™¨ï¼ˆEncoderï¼‰çš„ä¸€å±‚**

ç»“åˆäº† **å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰**ã€**å‰é¦ˆç½‘ç»œï¼ˆFeedForwardï¼‰**ã€**æ®‹å·®è¿æ¥ + LayerNorm + Dropout**ã€‚

#### 1. ç±»å®šä¹‰

```python
class EncoderLayer(nn.Module):
```

ç»§æ‰¿è‡ª `nn.Module`ï¼Œè¡¨ç¤º Transformer ç¼–ç å™¨çš„ä¸€å±‚ã€‚

#### 2. æ„é€ å‡½æ•°

```python
def __init__(self, size, self_attn, feed_forward, dropout):
    super(EncoderLayer, self).__init__()
    self.self_attn = self_attn
    self.feed_forward = feed_forward
    self.sublayer = clones(SublayerConnection(size, dropout), 2)
    self.size = size
```

å‚æ•°è¯´æ˜ï¼š

* `size`ï¼šè¾“å…¥ç‰¹å¾ç»´åº¦ `d_model`ã€‚
* `self_attn`ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼ˆ`MultiHeadedAttention`ï¼‰ã€‚
* `feed_forward`ï¼šä½ç½®å‰é¦ˆç½‘ç»œæ¨¡å—ï¼ˆ`PositionwiseFeedForward`ï¼‰ã€‚
* `dropout`ï¼šå­å±‚çš„ Dropout æ¦‚ç‡ã€‚

å…·ä½“åšäº†ï¼š

1. ä¿å­˜ **è‡ªæ³¨æ„åŠ›æ¨¡å—** å’Œ **å‰é¦ˆç½‘ç»œæ¨¡å—**ã€‚
2. ç”¨ `clones` åˆ›å»º **ä¸¤ä¸ª SublayerConnection**ï¼š

   * ç¬¬ä¸€ä¸ªå¯¹åº” **è‡ªæ³¨æ„åŠ›å­å±‚**
   * ç¬¬äºŒä¸ªå¯¹åº” **å‰é¦ˆç½‘ç»œå­å±‚**
3. ä¿å­˜è¾“å…¥ç»´åº¦ `size`ã€‚

#### 3. å‰å‘ä¼ æ’­

```python
def forward(self, x, mask):
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
    return self.sublayer[1](x, self.feed_forward)
```

**æ­¥éª¤æ‹†è§£ï¼š**

1. **ç¬¬ä¸€ä¸ªå­å±‚ï¼šè‡ªæ³¨æ„åŠ›**

```python
x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
```

* `sublayer[0]` æ˜¯ç¬¬ä¸€ä¸ª `SublayerConnection`ã€‚
  * è¾“å…¥å…ˆ LayerNormï¼Œç„¶åä¼ å…¥ lambda å‡½æ•°è°ƒç”¨ **å¤šå¤´è‡ªæ³¨æ„åŠ›**

* `lambda x: self.self_attn(x, x, x, mask)`ï¼šå®šä¹‰äº†ä¸€ä¸ªä¸´æ—¶å‡½æ•°ï¼Œè¾“å…¥æ˜¯ `x`ï¼Œè¾“å‡ºæ˜¯ `self.self_attn(x, x, x, mask)` çš„ç»“æœã€‚
  * åœ¨ Python é‡Œï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªå‡½æ•°æˆ–å¯¹è±¡æ–¹æ³• **æœ¬èº«éœ€è¦å¤šä¸ªå‚æ•°**ï¼Œä½†ä½ æƒ³æŠŠå®ƒä¼ ç»™ä¸€ä¸ª **åªä¼šä¼ ä¸€ä¸ªå‚æ•°çš„æ¥å£**ï¼Œå°±éœ€è¦ç”¨ **åŒ¿åå‡½æ•°ï¼ˆlambdaï¼‰æˆ– functools.partial** æ¥å°è£…ï¼ŒæŠŠé¢å¤–çš„å‚æ•°å›ºå®šä½ã€‚


```python
self.self_attn(x, x, x, mask)
```

* queryã€keyã€value éƒ½æ˜¯åŒä¸€ä¸ª `x` â†’ **è‡ªæ³¨æ„åŠ›**

  * è¿™é‡Œä¼ å…¥çš„ä¸‰ä¸ª x è¡¨ç¤º **æ¯ä¸ªå¤´çš„æŠ•å½±å‡½æ•°éƒ½ä¼šç‹¬ç«‹ç”Ÿæˆ Q/K/V**
  * å¹¶ä¸æ˜¯è¯´ Q=K=V=x
  * å†…éƒ¨å®ç°ï¼ˆåœ¨ MultiHeadedAttention é‡Œï¼‰æ˜¯ï¼š

  ```
  query = self.linears[0](x)
  key   = self.linears[1](x)
  value = self.linears[2](x)
  ```

  - æ‰€ä»¥ **ä¼ å…¥ç›¸åŒçš„ x** æ˜¯ä¸ºäº†è®©è‡ªæ³¨æ„åŠ›çŸ¥é“æ‰€æœ‰ä½ç½®çš„ä¿¡æ¯ï¼Œ**çº¿æ€§å±‚ä¼šç”Ÿæˆä¸åŒçš„ Q/K/V è¡¨ç¤º**
* ä½¿ç”¨ `mask` é¿å…å…³æ³¨åˆ°å¡«å……æˆ–æœªæ¥ä½ç½®
* æœ€ååšæ®‹å·®è¿æ¥å’Œ dropout

2. **ç¬¬äºŒä¸ªå­å±‚ï¼šå‰é¦ˆç½‘ç»œ**

```python
return self.sublayer[1](x, self.feed_forward)
```

* `sublayer[1]` æ˜¯ç¬¬äºŒä¸ª `SublayerConnection`ã€‚
* è¾“å…¥ `x` å…ˆ LayerNormï¼Œç„¶åä¼ ç»™ **å‰é¦ˆç½‘ç»œ**ï¼Œå†åšæ®‹å·®è¿æ¥å’Œ dropout

#### 4. æ€»ç»“

ä¸€å±‚ `EncoderLayer` çš„æµç¨‹ï¼š

```
è¾“å…¥ x
   â”‚
LayerNorm â†’ MultiHeadAttention â†’ Dropout â†’ æ®‹å·®ç›¸åŠ 
   â”‚
LayerNorm â†’ FeedForward â†’ Dropout â†’ æ®‹å·®ç›¸åŠ 
   â”‚
è¾“å‡º x
```

ç‰¹ç‚¹ï¼š

1. **ä¸¤ä¸ªå­å±‚**ï¼šè‡ªæ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ
2. **æ¯ä¸ªå­å±‚éƒ½æœ‰**ï¼šLayerNorm + Dropout + Residual
3. æ”¯æŒ maskï¼Œç”¨äºå±è”½ padding æˆ–æœªæ¥ä½ç½®

### Encoder

```python
class Encoder(nn.Module):
    def __init__(self, layer, N):
        super(Encoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, mask):
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
```

**Transformer ç¼–ç å™¨ï¼ˆEncoderï¼‰**

ç”± **N å±‚ç›¸åŒçš„ EncoderLayer å †å è€Œæˆ**ï¼Œå¹¶åœ¨æœ€ååŠ ä¸Š LayerNormã€‚

#### 1. ç±»å®šä¹‰

```python
class Encoder(nn.Module):
```

ç»§æ‰¿è‡ª `nn.Module`ï¼Œè¡¨ç¤ºæ•´ä¸ªç¼–ç å™¨æ¨¡å—ï¼Œä¸æ˜¯å•å±‚ï¼Œè€Œæ˜¯ç”±å¤šå±‚ EncoderLayer å †å æ„æˆã€‚

#### 2. æ„é€ å‡½æ•°

```python
def __init__(self, layer, N):
    super(Encoder, self).__init__()
    self.layers = clones(layer, N)
    self.norm = LayerNorm(layer.size)
```

å‚æ•°ï¼š

- `layer`ï¼šä¸€ä¸ª `EncoderLayer` å®ä¾‹
- `N`ï¼šå †å çš„å±‚æ•°ï¼ˆé€šå¸¸æ˜¯ 6 å±‚ï¼‰

åŠŸèƒ½ï¼š

1. **å…‹éš† N å±‚ç›¸åŒçš„ EncoderLayer**

```python
self.layers = clones(layer, N)
```

- `clones` è¿”å›ä¸€ä¸ªåŒ…å« N ä¸ª **ç‹¬ç«‹å‚æ•°å‰¯æœ¬** çš„åˆ—è¡¨ï¼Œæ¯ä¸€å±‚éƒ½æ˜¯å•ç‹¬å¯è®­ç»ƒçš„ã€‚
- æ¯ä¸€å±‚éƒ½æœ‰è‡ªå·±çš„å¤šå¤´è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œå‚æ•°ã€‚

1. **æœ€å LayerNorm**

```python
self.norm = LayerNorm(layer.size)
```

- `layer.size`ï¼šç”¨äºæœ€åçš„ LayerNorm
  - `size` å…¶å®å°±æ˜¯åœ¨ `EncoderLayer` çš„æ„é€ å‡½æ•°é‡Œï¼Œ **Transformer çš„éšè—ç»´åº¦ `d_model`**ï¼ˆä¾‹å¦‚ 512 æˆ– 1024ï¼‰
- åœ¨æ‰€æœ‰å±‚å †å å®Œæˆåï¼Œå¯¹æœ€ç»ˆè¾“å‡ºåš **å±‚å½’ä¸€åŒ–**ï¼Œä¿è¯æ•°å€¼ç¨³å®šã€‚

#### 3. å‰å‘ä¼ æ’­

```python
def forward(self, x, mask):
    for layer in self.layers:
        x = layer(x, mask)
    return self.norm(x)
```

æ­¥éª¤ï¼š

1. å¾ªç¯éå†æ¯ä¸€å±‚ `EncoderLayer`ï¼š

```python
x = layer(x, mask)
```

- æ¯ä¸€å±‚ä¼šæ‰§è¡Œï¼š
  - è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + LayerNorm
  - å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + LayerNorm
- è¾“å‡º `x` ä½œä¸ºä¸‹ä¸€å±‚è¾“å…¥

2. **æœ€å LayerNorm**

```python
return self.norm(x)
```

- å¯¹æœ€åä¸€å±‚çš„è¾“å‡ºåšä¸€æ¬¡å½’ä¸€åŒ–
- Transformer çš„è®ºæ–‡é‡Œæåˆ°æœ€åè¾“å‡ºéœ€è¦ LayerNormï¼ˆæœ‰äº›å®ç°ä¹Ÿå¯èƒ½æŠŠ LayerNorm æ”¾åœ¨æ¯å±‚ä¹‹å‰æˆ–ä¹‹åï¼‰

#### 4. æ€»ç»“

æ•´ä¸ª `Encoder` åšçš„äº‹æƒ…ï¼š

```
è¾“å…¥ x
   â”‚
EncoderLayer 1
   â”‚
EncoderLayer 2
   â”‚
...
   â”‚
EncoderLayer N
   â”‚
LayerNorm
   â”‚
è¾“å‡º x
```

- å †å  N å±‚ EncoderLayer
- æ¯å±‚åŒ…å«ï¼š
  - è‡ªæ³¨æ„åŠ› + æ®‹å·® + LayerNorm
  - å‰é¦ˆç½‘ç»œ + æ®‹å·® + LayerNorm
- è¾“å‡ºæ˜¯ç»è¿‡ LayerNorm çš„æœ€ç»ˆç¼–ç åºåˆ—è¡¨ç¤º

### DecoderLayer

```python
class DecoderLayer(nn.Module):
    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 3)

    def forward(self, x, memory, src_mask, tgt_mask):
        m = memory
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))
        return self.sublayer[2](x, self.feed_forward)
```

**Transformer è§£ç å™¨ï¼ˆDecoderï¼‰çš„ä¸€å±‚**

æ¯”ç¼–ç å™¨å±‚å¤šäº†ä¸€æ­¥ **å¯¹ç¼–ç å™¨è¾“å‡ºçš„æ³¨æ„åŠ›ï¼ˆEncoder-Decoder Attentionï¼‰**ã€‚

#### 1. ç±»å®šä¹‰

```python
class DecoderLayer(nn.Module):
```

ç»§æ‰¿è‡ª `nn.Module`ï¼Œè¡¨ç¤º Transformer è§£ç å™¨çš„ä¸€å±‚ã€‚

#### 2. æ„é€ å‡½æ•°

```python
def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
    super(DecoderLayer, self).__init__()
    self.size = size
    self.self_attn = self_attn
    self.src_attn = src_attn
    self.feed_forward = feed_forward
    self.sublayer = clones(SublayerConnection(size, dropout), 3)
```

å‚æ•°ï¼š

- `size`ï¼šéšè—ç»´åº¦ `d_model`
- `self_attn`ï¼šè§£ç å™¨è‡ªèº«çš„ **Masked Self-Attention**
- `src_attn`ï¼šè§£ç å™¨å¯¹ç¼–ç å™¨è¾“å‡ºçš„ **Encoder-Decoder Attention**
- `feed_forward`ï¼šå‰é¦ˆç½‘ç»œ
- `dropout`ï¼šDropout æ¦‚ç‡

åˆå§‹åŒ–ï¼š

1. ä¿å­˜ä¸‰ä¸ªå­æ¨¡å—ï¼š`self_attn`ã€`src_attn`ã€`feed_forward`
2. å…‹éš†ä¸‰ä¸ª `SublayerConnection`ï¼Œå¯¹åº”ä¸‰ä¸ªå­å±‚ï¼š
   1. Masked Self-Attention
   2. Encoder-Decoder Attention
   3. å‰é¦ˆç½‘ç»œ

#### 3. å‰å‘ä¼ æ’­

```python
def forward(self, x, memory, src_mask, tgt_mask):
    m = memory
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
    x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))
    return self.sublayer[2](x, self.feed_forward)
```

**æ­¥éª¤æ‹†è§£**

1. **Masked Self-Attentionï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ï¼‰**

```python
x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
```

- ä½œç”¨ï¼šè§£ç å™¨è‡ªå·±åºåˆ—å†…åšæ³¨æ„åŠ›
- **`tgt_mask`** é¿å…æ¨¡å‹çœ‹åˆ°æœªæ¥è¯ï¼ˆä¿è¯è‡ªå›å½’ç”Ÿæˆï¼‰
- ç»è¿‡ LayerNorm + Dropout + æ®‹å·®

1. **Encoder-Decoder Attention**

```python
x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))
```

- `x` æ˜¯è§£ç å™¨å½“å‰å±‚è¾“å…¥
- **`m = memory` æ˜¯ç¼–ç å™¨çš„è¾“å‡º**
- Query = `x`ï¼ˆè§£ç å™¨å½“å‰çŠ¶æ€ï¼‰
- Key/Value = `m`ï¼ˆç¼–ç å™¨è¾“å‡ºï¼‰
- **`src_mask`** ç”¨äºå±è”½ç¼–ç å™¨è¾“å…¥ä¸­çš„ padding
- ä½œç”¨ï¼šè®©è§£ç å™¨å…³æ³¨è¾“å…¥åºåˆ—ä¸­ç›¸å…³ä¿¡æ¯
- ç»è¿‡ LayerNorm + Dropout + æ®‹å·®

1. **å‰é¦ˆç½‘ç»œ**

```python
return self.sublayer[2](x, self.feed_forward)
```

- æ ‡å‡†å‰é¦ˆç½‘ç»œå¤„ç†æ¯ä¸ªä½ç½®çš„å‘é‡
- ç»è¿‡ LayerNorm + Dropout + æ®‹å·®

#### 4. æ€»ç»“

ä¸€å±‚ DecoderLayer æµç¨‹å¦‚ä¸‹ï¼š

```
è¾“å…¥ x
   â”‚
Masked Self-Attention (x->x) + Residual + LayerNorm
   â”‚
Encoder-Decoder Attention (x->memory) + Residual + LayerNorm
   â”‚
FeedForward + Residual + LayerNorm
   â”‚
è¾“å‡º x
```

ç‰¹ç‚¹ï¼š

1. **ä¸‰ä¸ªå­å±‚**ï¼šMasked Self-Attention â†’ Encoder-Decoder Attention â†’ å‰é¦ˆç½‘ç»œ
2. **æ¯ä¸ªå­å±‚éƒ½å¸¦æ®‹å·® + LayerNorm + Dropout**
3. é€šè¿‡ `lambda` å°è£…å­å±‚å‡½æ•°ï¼Œç»Ÿä¸€æ¥å£

### Decoder

```python
class Decoder(nn.Module):
    "Generic N layer decoder with masking."

    def __init__(self, layer, N):
        super(Decoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)
```

**Transformer è§£ç å™¨ï¼ˆDecoderï¼‰æ•´ä½“ç»“æ„**ï¼Œ

å’Œ `Encoder` å¾ˆåƒï¼Œä¸è¿‡æ¯ä¸€å±‚æ˜¯ `DecoderLayer`ï¼Œè€Œä¸” **forward çš„å‚æ•°å¤šäº† `memory`ï¼ˆç¼–ç å™¨è¾“å‡ºï¼‰å’Œ `mask`ã€‚**

#### 1. ç±»å®šä¹‰

```python
class Decoder(nn.Module):
    "Generic N layer decoder with masking."
```

* è¡¨ç¤ºæ•´ä¸ª Transformer è§£ç å™¨ï¼Œç”± **N å±‚ `DecoderLayer` å †å è€Œæˆ**ã€‚
* è¿™é‡Œçš„ `masking` æŒ‡çš„æ˜¯ **ç›®æ ‡åºåˆ—çš„æ©ç  (tgt\_mask)**ï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥è¯ã€‚

#### 2. æ„é€ å‡½æ•°

```python
def __init__(self, layer, N):
    super(Decoder, self).__init__()
    self.layers = clones(layer, N)
    self.norm = LayerNorm(layer.size)
```

å‚æ•°ï¼š

* `layer`ï¼šä¸€ä¸ª `DecoderLayer` å®ä¾‹
* `N`ï¼šå †å å±‚æ•°ï¼ˆé€šå¸¸æ˜¯ 6ï¼‰

é€»è¾‘ï¼š

1. `self.layers = clones(layer, N)`

   * å…‹éš† N ä¸ª `DecoderLayer`ï¼Œæ¯å±‚éƒ½æœ‰ç‹¬ç«‹çš„å‚æ•°ï¼ˆè‡ªæ³¨æ„åŠ›ã€äº¤å‰æ³¨æ„åŠ›ã€å‰é¦ˆç½‘ç»œï¼‰ã€‚
2. `self.norm = LayerNorm(layer.size)`

   * åœ¨æœ€åè¾“å‡ºå‰åšä¸€æ¬¡ LayerNormï¼Œä¿è¯æ•°å€¼ç¨³å®šã€‚

#### 3. å‰å‘ä¼ æ’­

```python
def forward(self, x, memory, src_mask, tgt_mask):
    for layer in self.layers:
        x = layer(x, memory, src_mask, tgt_mask)
    return self.norm(x)
```

è¾“å…¥å‚æ•°ï¼š

* `x`ï¼šç›®æ ‡åºåˆ—ï¼ˆdecoder è¾“å…¥ï¼Œæ¯”å¦‚è®­ç»ƒæ—¶æ˜¯ `<sos> + y` çš„åµŒå…¥è¡¨ç¤ºï¼‰
* `memory`ï¼šç¼–ç å™¨çš„è¾“å‡ºï¼ˆä½œä¸º cross-attention çš„ Key/Valueï¼‰
* `src_mask`ï¼šæºåºåˆ— maskï¼ˆå±è”½æ‰ padding éƒ¨åˆ†ï¼‰
* `tgt_mask`ï¼šç›®æ ‡åºåˆ— maskï¼ˆä¿è¯è§£ç å™¨ä¸èƒ½å·çœ‹æœªæ¥ tokenï¼‰

æµç¨‹ï¼š

1. ä¾æ¬¡é€šè¿‡ N å±‚ `DecoderLayer`

   * æ¯å±‚éƒ½ä¼šåšï¼š

     1. Masked Self-Attentionï¼ˆå¯¹è‡ªå·±ç›®æ ‡åºåˆ—å†…éƒ¨åšæ³¨æ„åŠ›ï¼‰
     2. Encoder-Decoder Attentionï¼ˆå¯¹ç¼–ç å™¨è¾“å‡ºåšæ³¨æ„åŠ›ï¼‰
     3. å‰é¦ˆç½‘ç»œ
   * æ¯ä¸ªå­å±‚éƒ½æœ‰æ®‹å·® + LayerNorm
2. æœ€ååšä¸€æ¬¡ LayerNorm

è¾“å‡ºï¼š

* å½¢çŠ¶ `(batch_size, tgt_seq_len, d_model)`
* å³ç›®æ ‡åºåˆ—æ¯ä¸ªä½ç½®çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼ŒåŒ…å«äº†ç›®æ ‡åºåˆ—å†…éƒ¨çš„ä¾èµ– + æºåºåˆ—çš„ä¿¡æ¯

#### 4. ç»“æ„æ€»ç»“

`Decoder` çš„æ•´ä½“ç»“æ„ï¼š

```
è¾“å…¥ x (ç›®æ ‡åºåˆ—åµŒå…¥)
   â”‚
  N å±‚ DecoderLayer å †å 
   â”‚
æ¯å±‚ = [Masked Self-Attn â†’ Cross-Attn (with memory) â†’ FeedForward]
   â”‚
LayerNorm
   â”‚
è¾“å‡º (è§£ç åºåˆ—çš„è¡¨ç¤º)
```

### EncoderDecoder

```python
class EncoderDecoder(nn.Module):
    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.generator = generator

    def forward(self, src, tgt, src_mask, tgt_mask):
        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)

    def encode(self, src, src_mask):
        return self.encoder(self.src_embed(src), src_mask)

    def decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
```

**Transformer çš„é¡¶å±‚æ¨¡å‹**

#### 1. `__init__`

```python
def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
```

* **encoder**ï¼šç¼–ç å™¨ï¼ˆç”± `Encoder` ç±»æ„é€ ï¼ŒåŒ…å« N å±‚ï¼‰
* **decoder**ï¼šè§£ç å™¨ï¼ˆç”± `Decoder` ç±»æ„é€ ï¼ŒåŒ…å« N å±‚ï¼‰
* **src\_embed**ï¼šæºåºåˆ—åµŒå…¥ï¼ˆé€šå¸¸æ˜¯ `nn.Embedding + PositionalEncoding`ï¼‰
* **tgt\_embed**ï¼šç›®æ ‡åºåˆ—åµŒå…¥ï¼ˆåŒä¸Šï¼‰
* **generator**ï¼šæœ€åçš„è¾“å‡ºå±‚ï¼ˆæŠŠ decoder çš„éšçŠ¶æ€æ˜ å°„åˆ°è¯è¡¨æ¦‚ç‡åˆ†å¸ƒï¼Œå¸¸ç”¨ `nn.Linear(d_model, vocab_size) + Softmax`ï¼‰

#### 2. `forward`

```python
def forward(self, src, tgt, src_mask, tgt_mask):
    return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)
```

è¿™é‡Œæ‰§è¡Œçš„æ­¥éª¤å°±æ˜¯ Transformer çš„ **æ•´ä½“æµç¨‹**ï¼š

1. **encode**

   ```python
   memory = self.encode(src, src_mask)
   ```

   * è¾“å…¥ï¼š`src`ï¼ˆæºåºåˆ— token idï¼‰ï¼Œ`src_mask`ï¼ˆå¯¹ pad ä½ç½®çš„æ©ç ï¼‰
   * æ“ä½œï¼šå…ˆåµŒå…¥ï¼ˆ`src_embed`ï¼‰ï¼Œå†é€å…¥ encoder
   * è¾“å‡ºï¼š`memory`ï¼Œå³ç¼–ç å™¨è¾“å‡ºï¼ˆæ‰€æœ‰æº token çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼‰

2. **decode**

   ```python
   out = self.decode(memory, src_mask, tgt, tgt_mask)
   ```

   * è¾“å…¥ï¼š

     * `memory`ï¼šæ¥è‡ª encoder çš„ä¸Šä¸‹æ–‡
     * `src_mask`ï¼šæºåºåˆ—æ©ç ï¼ˆä¿è¯å¯¹é½ï¼‰
     * `tgt`ï¼šç›®æ ‡åºåˆ— token idï¼ˆè®­ç»ƒæ—¶è¾“å…¥å·²çŸ¥çš„å‰ç¼€ï¼Œé¢„æµ‹æ—¶è¾“å…¥å·²ç”Ÿæˆçš„ tokenï¼‰
     * `tgt_mask`ï¼šç›®æ ‡æ©ç ï¼ˆé˜²æ­¢ decoder çœ‹è§æœªæ¥ tokenï¼‰
   * æ“ä½œï¼šå…ˆåµŒå…¥ï¼ˆ`tgt_embed`ï¼‰ï¼Œå†é€å…¥ decoder
   * è¾“å‡ºï¼šè§£ç å™¨çš„éšçŠ¶æ€è¡¨ç¤º

#### 3. `encode`

```python
def encode(self, src, src_mask):
    return self.encoder(self.src_embed(src), src_mask)
```

* `src_embed(src)`ï¼šè¯åµŒå…¥ + ä½ç½®ç¼–ç 
* `self.encoder(...)`ï¼šN å±‚ encoder å †å 
* è¾“å‡ºï¼šmemory (ç»´åº¦ `[batch_size, src_len, d_model]`)

#### 4. `decode`

```python
def decode(self, memory, src_mask, tgt, tgt_mask):
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
```

* `tgt_embed(tgt)`ï¼šç›®æ ‡åºåˆ—åµŒå…¥ + ä½ç½®ç¼–ç 
* `self.decoder(...)`ï¼šN å±‚ decoder å †å 
* è¾“å…¥ï¼šmemoryï¼ˆencoder è¾“å‡ºï¼‰ã€src\_maskã€tgt\_mask
* è¾“å‡ºï¼šdecoder éšçŠ¶æ€ï¼ˆè¿˜æ²¡è¿‡æœ€åçš„ `generator`ï¼‰

### Generator

```python
class Generator(nn.Module):
    def __init__(self, d_model, vocab):
        super(Generator, self).__init__()
        self.proj = nn.Linear(d_model, vocab)

    def forward(self, x):
        # x: [batch, seq_len, d_model]
        # è¾“å‡º: [batch, seq_len, vocab]ï¼Œå†ç»è¿‡ softmax æ‰æ˜¯æ¦‚ç‡åˆ†å¸ƒ
        return F.log_softmax(self.proj(x), dim=-1)
```

**`Generator` ç±»**å°±æ˜¯ **è¾“å‡ºå±‚**

å®ƒæŠŠ decoder çš„éšè—çŠ¶æ€è½¬æˆå¯¹è¯è¡¨çš„æ¦‚ç‡åˆ†å¸ƒã€‚

#### 1. æ„é€ å‡½æ•°

```python
class Generator(nn.Module):
    def __init__(self, d_model, vocab):
        super(Generator, self).__init__()
        self.proj = nn.Linear(d_model, vocab)
```

- `d_model`ï¼šdecoder æ¯ä¸ª token çš„è¡¨ç¤ºç»´åº¦ï¼ˆæ¯”å¦‚ 512ï¼‰
- `vocab`ï¼šç›®æ ‡è¯­è¨€è¯è¡¨å¤§å°ï¼ˆæ¯”å¦‚ 37000ï¼‰
- `nn.Linear(d_model, vocab)`ï¼šæŠŠ `[batch, seq_len, d_model]` çš„å‘é‡æŠ•å½±åˆ° `[batch, seq_len, vocab]`ï¼Œç›¸å½“äºä¸ºæ¯ä¸ªä½ç½®ç”Ÿæˆä¸€ä¸ª **è¯è¡¨æ‰“åˆ†å‘é‡**ã€‚

#### 2. å‰å‘ä¼ æ’­

```python
def forward(self, x):
    # x: [batch, seq_len, d_model]
    return F.log_softmax(self.proj(x), dim=-1)
```

- è¾“å…¥ `x`ï¼šdecoder è¾“å‡ºçš„éšè—çŠ¶æ€
   â†’ shape `[batch, seq_len, d_model]`
- `self.proj(x)`ï¼šçº¿æ€§æ˜ å°„åˆ°è¯è¡¨ç»´åº¦
   â†’ shape `[batch, seq_len, vocab]`
- `F.log_softmax(..., dim=-1)`ï¼šåœ¨ **æœ€åä¸€ç»´ï¼ˆè¯è¡¨ç»´åº¦ï¼‰** ä¸Šåš log-softmaxï¼Œå¾—åˆ° **log æ¦‚ç‡åˆ†å¸ƒ**ã€‚

#### 3. ä¸ºä»€ä¹ˆç”¨ `log_softmax` è€Œä¸æ˜¯ `softmax`ï¼Ÿ

- `softmax` ç»™çš„æ˜¯æ¦‚ç‡åˆ†å¸ƒ (0~1)
- `log_softmax` ç»™çš„æ˜¯å¯¹æ•°æ¦‚ç‡
- åœ¨è®­ç»ƒæ—¶é€šå¸¸ç”¨ **`nn.NLLLoss()`**ï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼‰ï¼Œå®ƒå’Œ `log_softmax` æ­£å¥½æ­é…ä½¿ç”¨ã€‚
   ï¼ˆå¦åˆ™å°±è¦ `nn.CrossEntropyLoss()`ï¼Œå®ƒå†…éƒ¨ä¼šè‡ªå·±åš `log_softmax`ï¼‰

#### 4. æ€»ç»“

- `Generator` = **è¾“å‡ºå±‚**
- ä½œç”¨ï¼šæŠŠ decoder çš„éšè—çŠ¶æ€ â†’ è¯è¡¨åˆ†å¸ƒ
- æµç¨‹ï¼š
   `decoder è¾“å‡º (d_model)` â†’ `Linear` â†’ `[vocab]` â†’ `log_softmax`

### make_model

```python
def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):
    c = copy.deepcopy
    attn = MultiHeadedAttention(h, d_model)
    ff = PositionwiseFeedForward(d_model, d_ff, dropout)
    position = PositionalEncoding(d_model, dropout)
    model = EncoderDecoder(
        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
        Decoder(DecoderLayer(d_model, c(attn), c(attn),c(ff), dropout), N),
        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),
        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),
        Generator(d_model, tgt_vocab))
```

**æ¨¡å‹æ„å»ºå‡½æ•° `make_model`** 

è¿™æ˜¯ã€ŠAttention Is All You Needã€‹è®ºæ–‡é‡Œ Transformer çš„ç»å…¸å®ç°æ–¹å¼ã€‚

#### 1. å‚æ•°

```python
def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):
```

- **src_vocab**ï¼šæºè¯­è¨€è¯è¡¨å¤§å°ï¼ˆintï¼Œæ¯”å¦‚è‹±è¯­ 37000ï¼‰
- **tgt_vocab**ï¼šç›®æ ‡è¯­è¨€è¯è¡¨å¤§å°ï¼ˆintï¼Œæ¯”å¦‚å¾·è¯­ 37000ï¼‰
- **N**ï¼šencoder å’Œ decoder çš„å±‚æ•°ï¼ˆé»˜è®¤ 6 å±‚ï¼‰
- **d_model**ï¼šembedding å’Œéšè—å‘é‡çš„ç»´åº¦ï¼ˆé»˜è®¤ 512ï¼‰
- **d_ff**ï¼šå‰é¦ˆå…¨è¿æ¥å±‚éšè—ç»´åº¦ï¼ˆé»˜è®¤ 2048ï¼‰
- **h**ï¼šå¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°ï¼ˆé»˜è®¤ 8ï¼‰
- **dropout**ï¼šdropout æ¦‚ç‡

#### 2. å…‹éš†å‡½æ•°

```python
c = copy.deepcopy
```

âš¡ ä½œç”¨ï¼šå› ä¸ºåŒä¸€ä¸ª `attn` æˆ– `ff` å¯¹è±¡ä¸èƒ½ç›´æ¥å¤ç”¨åˆ°å¤šä¸ªå±‚ï¼ˆå¦åˆ™å…±äº«æƒé‡ï¼‰ï¼Œ
 æ‰€ä»¥ç”¨ `deepcopy` æ‹·è´å‡º **å‚æ•°ç‹¬ç«‹**çš„å¤šä¸ªå‰¯æœ¬ã€‚

#### 3. å®šä¹‰å­æ¨¡å—

```python
attn = MultiHeadedAttention(h, d_model)
ff = PositionwiseFeedForward(d_model, d_ff, dropout)
position = PositionalEncoding(d_model, dropout)
```

- **attn**ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼ˆè¾“å…¥ `x`ï¼Œå†…éƒ¨ç”Ÿæˆ Q, K, Vï¼‰
- **ff**ï¼šé€ä½ç½®å‰é¦ˆç½‘ç»œï¼ˆä¸¤ä¸ª `Linear`ï¼Œä¸­é—´ `ReLU`ï¼‰
- **position**ï¼šä½ç½®ç¼–ç ï¼ˆsin/cos å½¢å¼ + dropoutï¼‰

#### 4. ç»„è£… Encoder

```python
Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)
```

- `EncoderLayer`ï¼šä¸€å±‚ = **è‡ªæ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ**
- `Encoder`ï¼šå †å  N å±‚
- æ¯å±‚éƒ½éœ€è¦ç‹¬ç«‹çš„ `attn` å’Œ `ff` â†’ ç”¨ `c(...)` æ‹·è´

#### 5. ç»„è£… Decoder

```python
Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N)
```

- `DecoderLayer`ï¼šä¸€å±‚ = **è‡ªæ³¨æ„åŠ› + ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ**
  - ç¬¬ä¸€ä¸ª `attn`ï¼šç›®æ ‡åºåˆ—çš„ **è‡ªæ³¨æ„åŠ›**ï¼ˆmaskedï¼‰
  - ç¬¬äºŒä¸ª `attn`ï¼šå¯¹ç¼–ç å™¨ `memory` çš„ **äº¤äº’æ³¨æ„åŠ›**
  - `ff`ï¼šé€ä½ç½®å‰é¦ˆç½‘ç»œ
- `Decoder`ï¼šå †å  N å±‚

#### 6. åµŒå…¥ + ä½ç½®ç¼–ç 

```python
nn.Sequential(Embeddings(d_model, src_vocab), c(position))
nn.Sequential(Embeddings(d_model, tgt_vocab), c(position))
```

- **Embeddings**ï¼šæŠŠ token id â†’ d_model ç»´åº¦å‘é‡
- **PositionalEncoding**ï¼šç»™ embedding åŠ ä½ç½®ä¿¡æ¯
- `Sequential`ï¼šç»„åˆæˆä¸€ä¸ªæ•´ä½“æ¨¡å—

#### 7. è¾“å‡ºå±‚

```python
Generator(d_model, tgt_vocab)
```

- æŠŠ decoder è¾“å‡º `[batch, len, d_model]`
   æ˜ å°„åˆ° `[batch, len, tgt_vocab]`ï¼Œå¾—åˆ° softmax æ¦‚ç‡åˆ†å¸ƒã€‚

#### 8. è¿”å›æ¨¡å‹

```python
model = EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator)
```

æ•´ä½“æ‹¼å¥½ï¼Œå¾—åˆ°å®Œæ•´çš„ Transformerã€‚

#### æ€»ç»“

 `make_model` å°±æ˜¯æŠŠ **Transformer æ¶æ„**æ‹¼è£…èµ·æ¥ï¼Œå‚æ•°æ„ä¹‰å¦‚ä¸‹ï¼š

- `src_vocab` / `tgt_vocab`ï¼šè¾“å…¥/è¾“å‡ºè¯è¡¨å¤§å°
- `d_model`ï¼šembedding ç»´åº¦
- `h`ï¼šå¤šå¤´æ•°
- `d_ff`ï¼šå‰é¦ˆå±‚å¤§å°
- `N`ï¼šå±‚æ•°
- `dropout`ï¼šé˜²è¿‡æ‹Ÿåˆ

### æ•´ä½“ä»£ç 

Modular_Transformer.py

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import copy
from torch.autograd import Variable


def clones(module, N):
    "Produce N identical layers."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

def attention(query, key, value, mask=None, dropout=None):
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = F.softmax(scores, dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn

class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value, mask=None):
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)

        # 1) Do all the linear projections in batch from d_model => h x d_k
        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]
        # 2) Apply attention on all the projected vectors in batch.
        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)

        # 3) "Concat" using a view and apply a final linear.
        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
        return self.linears[-1](x)

class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))

class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model

    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) *
                             -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)
        return self.dropout(x)

class LayerNorm(nn.Module):
    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2

class SublayerConnection(nn.Module):

    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm(x)))

class EncoderLayer(nn.Module):
    def __init__(self, size, self_attn, feed_forward, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 2)
        self.size = size

    def forward(self, x, mask):
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
        return self.sublayer[1](x, self.feed_forward)

class Encoder(nn.Module):
    def __init__(self, layer, N):
        super(Encoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, mask):
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)

class DecoderLayer(nn.Module):
    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 3)

    def forward(self, x, memory, src_mask, tgt_mask):
        m = memory
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))
        return self.sublayer[2](x, self.feed_forward)

class Decoder(nn.Module):
    "Generic N layer decoder with masking."

    def __init__(self, layer, N):
        super(Decoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)

class EncoderDecoder(nn.Module):
    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.generator = generator

    def forward(self, src, tgt, src_mask, tgt_mask):
        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)

    def encode(self, src, src_mask):
        return self.encoder(self.src_embed(src), src_mask)

    def decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)

class Generator(nn.Module):
    def __init__(self, d_model, vocab):
        super(Generator, self).__init__()
        self.proj = nn.Linear(d_model, vocab)

    def forward(self, x):
        # x: [batch, seq_len, d_model]
        # è¾“å‡º: [batch, seq_len, vocab]ï¼Œå†ç»è¿‡ softmax æ‰æ˜¯æ¦‚ç‡åˆ†å¸ƒ
        return F.log_softmax(self.proj(x), dim=-1)

def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):
    c = copy.deepcopy
    attn = MultiHeadedAttention(h, d_model)
    ff = PositionwiseFeedForward(d_model, d_ff, dropout)
    position = PositionalEncoding(d_model, dropout)
    model = EncoderDecoder(
        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
        Decoder(DecoderLayer(d_model, c(attn), c(attn),c(ff), dropout), N),
        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),
        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),
        Generator(d_model, tgt_vocab)
    )
    return model
```



## è®­ç»ƒ

```python
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
from Modular_Transformer import make_model
from tqdm import tqdm

# =======================
# 1. æ•°æ®é›†ç±»ï¼ˆwmt16 ç‰ˆæœ¬ï¼‰
# =======================
class WMT16Dataset(Dataset):
    def __init__(self, split="train", src_lang="de", tgt_lang="en", max_len=50, max_samples=500, data_dir="./data"):
        dataset = load_dataset("wmt16", "de-en", split=split, cache_dir=data_dir)
        if max_samples:
            dataset = dataset.select(range(max_samples))

        # âš ï¸ æ³¨æ„è¿™é‡Œè®¿é—® translation å­—æ®µ
        self.src_sentences = [ex['translation'][src_lang] for ex in dataset]
        self.tgt_sentences = [ex['translation'][tgt_lang] for ex in dataset]
        self.max_len = max_len

        # æ„å»ºè¯è¡¨
        # è¯è¡¨å¤§å° = æ‰€æœ‰è®­ç»ƒå¥å­ä¸­ä¸é‡å¤çš„ token æ•° + ç‰¹æ®Š tokenã€‚
        # åŸæœ¬çš„æ¯ä¸ªtokenå°±æ˜¯åœ¨è¯è¡¨ä¸­çš„ä¸€ä¸ªæ•°å­—ï¼Œé€šè¿‡embeddingåï¼Œå˜ä¸ºd_modelé•¿åº¦çš„å‘é‡äº†
        self.src_vocab = self.build_vocab(self.src_sentences)
        self.tgt_vocab = self.build_vocab(self.tgt_sentences)

    @staticmethod
    def build_vocab(sentences):
        vocab = {"<pad>":0, "<sos>":1, "<eos>":2}
        idx = 3
        for s in sentences:
            for w in s.strip().split():
                if w not in vocab:
                    vocab[w] = idx
                    idx += 1
        return vocab

    def encode_sentence(self, sentence, vocab):
        tokens = ["<sos>"] + sentence.strip().split() + ["<eos>"]
        ids = [vocab.get(t, 0) for t in tokens]
        if len(ids) < self.max_len:
            ids += [vocab["<pad>"]] * (self.max_len - len(ids))
        return ids[:self.max_len]

    def __len__(self):
        return len(self.src_sentences)

    def __getitem__(self, idx):
        src_ids = torch.tensor(self.encode_sentence(self.src_sentences[idx], self.src_vocab))
        tgt_ids = torch.tensor(self.encode_sentence(self.tgt_sentences[idx], self.tgt_vocab))
        return src_ids, tgt_ids

# =======================
# 2. Collate å‡½æ•°
# =======================
def collate_fn(batch):
    src_batch, tgt_batch = zip(*batch)
    src_batch = torch.stack(src_batch)
    tgt_batch = torch.stack(tgt_batch)
    return src_batch, tgt_batch

# =======================
# 3. Mask å‡½æ•°
# =======================
def create_masks(src, tgt, pad_idx):
    src_mask = (src != pad_idx).unsqueeze(1)
    tgt_len = tgt.size(1)
    tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=src.device)).bool()
    tgt_padding_mask = (tgt != pad_idx).unsqueeze(1)
    tgt_mask = tgt_mask & tgt_padding_mask
    return src_mask, tgt_mask

# =======================
# 4. è®­ç»ƒå‡½æ•°
# =======================
def train(model, dataset, epochs=5, lr=1e-3, batch_size=32, save_dir="model", device=None):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    os.makedirs(save_dir, exist_ok=True)

    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss(ignore_index=dataset.tgt_vocab["<pad>"])

    start_epoch = 0
    save_path = os.path.join(save_dir, "transformer_wmt16.pth")

    if os.path.exists(save_path):
        checkpoint = torch.load(save_path, map_location=device)
        model.load_state_dict(checkpoint["model_state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        start_epoch = checkpoint["epoch"] + 1
        print(f"ğŸ”„ Loaded model from epoch {checkpoint['epoch']+1}, loss={checkpoint['loss']:.4f}")

    for epoch in range(start_epoch, epochs):
        model.train()
        total_loss = 0
        # ä½¿ç”¨ tqdm åŒ…è£¹ DataLoaderï¼Œæ˜¾ç¤ºè¿›åº¦
        loop = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{epochs}")
        for src, tgt in loop:
            src, tgt = src.to(device), tgt.to(device)
            src_mask, tgt_mask = create_masks(src, tgt[:, :-1], dataset.src_vocab["<pad>"])

            optimizer.zero_grad()
            output = model(src, tgt[:, :-1], src_mask, tgt_mask)
            logits = model.generator(output)
            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1))
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

            # æ›´æ–°è¿›åº¦æ¡æè¿°æ˜¾ç¤ºå½“å‰ loss
            loop.set_postfix(loss=loss.item())

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1}/{epochs}, Loss = {avg_loss:.4f}")

        torch.save({
            "epoch": epoch,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "loss": avg_loss
        }, save_path)
        print(f"ğŸ’¾ Model saved at {save_path}")

# =======================
# 5. æ¨ç†å‡½æ•°
# =======================
def translate(model, sentence, dataset, max_len=50, device=None):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model.eval()
    with torch.no_grad():
        src_ids = torch.tensor(dataset.encode_sentence(sentence, dataset.src_vocab)).unsqueeze(0).to(device)
        src_mask = (src_ids != dataset.src_vocab["<pad>"]).unsqueeze(1).to(device)
        tgt_ids = torch.tensor([[dataset.tgt_vocab["<sos>"]]], device=device)

        for _ in range(max_len):
            tgt_mask = torch.tril(torch.ones(tgt_ids.size(1), tgt_ids.size(1), device=device)).bool().unsqueeze(0)
            out = model(src_ids, tgt_ids, src_mask, tgt_mask)
            logits = model.generator(out)
            next_word = logits[:, -1, :].argmax(dim=-1).unsqueeze(0)
            tgt_ids = torch.cat([tgt_ids, next_word], dim=1)
            if next_word.item() == dataset.tgt_vocab["<eos>"]:
                break

        id_to_word = {i: w for w, i in dataset.tgt_vocab.items()}
        words = [id_to_word[i.item()] for i in tgt_ids[0][1:]]
        if "<eos>" in words:
            words = words[:words.index("<eos>")]
        return " ".join(words)

# =======================
# 6. ä¸»ç¨‹åº
# =======================
if __name__ == "__main__":
    dataset = WMT16Dataset(split="train", src_lang="de", tgt_lang="en", max_len=500, max_samples=5000, data_dir="./data")
    '''
    å‚æ•°è¯´æ˜ï¼š
    split="train"     -> ä½¿ç”¨è®­ç»ƒé›†ï¼ˆtrainï¼‰ï¼Œå¯é€‰ "validation" æˆ– "test"    
        train       454885æ¡
        validation  2169æ¡
        test        2999æ¡
    src_lang="de"     -> æºè¯­è¨€ä¸ºå¾·è¯­ï¼Œæ¨¡å‹è¾“å…¥ä¸ºå¾·è¯­å¥å­
    tgt_lang="en"     -> ç›®æ ‡è¯­è¨€ä¸ºè‹±è¯­ï¼Œæ¨¡å‹è¾“å‡ºä¸ºè‹±è¯­å¥å­
    max_len=50        -> æ¯ä¸ªå¥å­æœ€å¤§é•¿åº¦ï¼Œè¶…è¿‡æˆªæ–­ï¼Œä¸è¶³è¡¥ <pad>
    max_samples=5000  -> æœ€å¤§å–æ ·æ•°é‡ä¸º 5000 æ¡ï¼Œç”¨äºå¿«é€Ÿè°ƒè¯•
    data_dir="./data" -> æ•°æ®ä¸‹è½½/ç¼“å­˜åˆ°é¡¹ç›®ç›®å½•ä¸‹çš„ ./data æ–‡ä»¶å¤¹
    '''

    model = make_model(len(dataset.src_vocab), len(dataset.tgt_vocab), N=2, d_model=128, d_ff=256, h=4)
    '''
    å‚æ•°è¯´æ˜ï¼š
    len(dataset.src_vocab) -> æºè¯­è¨€è¯è¡¨å¤§å°ï¼ˆå¾·è¯­ï¼‰ï¼Œç”¨äºåµŒå…¥å±‚è¾“å…¥ç»´åº¦
    len(dataset.tgt_vocab) -> ç›®æ ‡è¯­è¨€è¯è¡¨å¤§å°ï¼ˆè‹±è¯­ï¼‰ï¼Œç”¨äºè¾“å‡ºå±‚ç”Ÿæˆæ¦‚ç‡
    N=2   -> Encoder å’Œ Decoder å †å çš„å±‚æ•°ï¼Œæ¯ä¸ªå †å åŒ…å«å¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œ
    d_model=128 -> Transformer ä¸­éšè—çŠ¶æ€å‘é‡ç»´åº¦ï¼Œä¹Ÿæ˜¯åµŒå…¥å‘é‡ç»´åº¦
    d_ff=256    -> å‰é¦ˆå…¨è¿æ¥å±‚çš„ç»´åº¦ï¼ˆFeed-Forward Networkï¼‰
    h=4        -> å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å¤´æ•°ï¼ˆhead æ•°ï¼‰ï¼Œæ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›
    '''

    train(model, dataset, epochs=30, batch_size=32, save_dir="model")


    while True:
        sentence = input("Enter German sentence (or 'quit' to exit): ")
        if sentence.lower() == "quit":
            break
        translation = translate(model, sentence, dataset)
        print("Translation:", translation)
    '''
    data = [
        ("i like cats", "ich mag katzen"),
        ("you love dogs", "du liebst hunde"),
        ("he eats food", "er isst essen"),
        ("we play games", "wir spielen spiele"),
        ("they read books", "sie lesen bÃ¼cher"),
    ]
    '''
```

### create_masks

```python
def create_masks(src, tgt, pad_idx):
    # 1. å¯¹ source åºåˆ—åˆ›å»º padding mask
    src_mask = (src != pad_idx).unsqueeze(1)

    # 2. ç›®æ ‡åºåˆ—é•¿åº¦
    tgt_len = tgt.size(1)

    # 3. ç”Ÿæˆä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µï¼ˆé˜²æ­¢é¢„æµ‹æ—¶çœ‹åˆ°æœªæ¥çš„ä¿¡æ¯ï¼‰
    tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=src.device)).bool()

    # 4. å¯¹ target åºåˆ—ä¹ŸåŠ ä¸Š padding mask
    tgt_padding_mask = (tgt != pad_idx).unsqueeze(1)

    # 5. å°†æœªæ¥ä½ç½®çš„ mask å’Œ padding mask ç»“åˆ
    tgt_mask = tgt_mask & tgt_padding_mask

    return src_mask, tgt_mask
```

`create_masks` å‡½æ•°ã€‚

è¿™ä¸ªå‡½æ•°çš„ç›®çš„æ˜¯ä¸º **Transformer æ¨¡å‹** ç”Ÿæˆ **Mask æ©ç **ï¼Œé¿å…æ¨¡å‹åœ¨è®­ç»ƒæ—¶çœ‹åˆ°ä¸è¯¥çœ‹åˆ°çš„ä¿¡æ¯ï¼ˆæ¯”å¦‚ paddingã€æœªæ¥çš„ tokenï¼‰ã€‚

#### è¯¦ç»†è§£é‡Šï¼š

1. `src_mask = (src != pad_idx).unsqueeze(1)`

* `src != pad_idx`ï¼šç”Ÿæˆä¸€ä¸ªå¸ƒå°”å¼ é‡ï¼Œè¡¨ç¤ºå“ªäº›ä½ç½®ä¸æ˜¯ `<PAD>`ã€‚

  * å‡è®¾ `pad_idx = 0`ï¼Œ`src = [2, 5, 0, 0]`
  * å¾—åˆ° `[True, True, False, False]`
* `unsqueeze(1)`ï¼šåœ¨ç¬¬ 1 ä¸ªç»´åº¦å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œä¾¿äºåç»­å’Œæ³¨æ„åŠ›è®¡ç®—æ—¶åšå¹¿æ’­ã€‚
* **ä½œç”¨**ï¼šå‘Šè¯‰æ¨¡å‹ `src` é‡Œé¢å“ªäº› token æ˜¯æœ‰æ•ˆçš„ã€‚

2. `tgt_len = tgt.size(1)`

* è·å–ç›®æ ‡åºåˆ—çš„é•¿åº¦ã€‚
* å‡è®¾ `tgt = [[4, 7, 9, 0, 0]]`ï¼Œ`tgt_len = 5`ã€‚

3. `tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=src.device)).bool()`

* `torch.ones(tgt_len, tgt_len)` ç”Ÿæˆä¸€ä¸ªå…¨ 1 çŸ©é˜µã€‚
* `torch.tril(...)` å–ä¸‹ä¸‰è§’çŸ©é˜µã€‚

  * ä¸¾ä¾‹ï¼š`tgt_len=5`

  ```
  [[1, 0, 0, 0, 0],
   [1, 1, 0, 0, 0],
   [1, 1, 1, 0, 0],
   [1, 1, 1, 1, 0],
   [1, 1, 1, 1, 1]]
  ```
* `bool()` è½¬ä¸ºå¸ƒå°”ç±»å‹ã€‚
* **ä½œç”¨**ï¼šè®© decoder åœ¨é¢„æµ‹ç¬¬ t ä¸ª token æ—¶ï¼Œåªèƒ½çœ‹åˆ° `t` åŠå…¶ä¹‹å‰çš„ tokenï¼Œé¿å…ä¿¡æ¯æ³„éœ²ï¼ˆcausal maskï¼‰ã€‚

4. `tgt_padding_mask = (tgt != pad_idx).unsqueeze(1)`

* å’Œ `src_mask` ç±»ä¼¼ï¼Œä½†ä½œç”¨åœ¨ `tgt`ã€‚
* æ ‡è®°å‡º `tgt` ä¸­å“ªäº›ä½ç½®æ˜¯æœ‰æ•ˆ tokenï¼Œå“ªäº›æ˜¯ `<PAD>`ã€‚

5. `tgt_mask = tgt_mask & tgt_padding_mask`

* å°† **æœªæ¥ä¿¡æ¯å±è”½ mask (causal mask)** å’Œ **padding mask** ç»“åˆã€‚
* æœ€ç»ˆçš„ `tgt_mask` æ—¢èƒ½ä¿è¯æ¨¡å‹ä¸ä¼šçœ‹åˆ°æœªæ¥çš„ tokenï¼Œä¹Ÿä¸ä¼šå¤„ç† `<PAD>`ã€‚

#### è¿”å›ç»“æœï¼š

* `src_mask`ï¼šæ ‡è®°è¾“å…¥åºåˆ—ä¸­å“ªäº› token æœ‰æ•ˆã€‚
* `tgt_mask`ï¼šæ ‡è®°ç›®æ ‡åºåˆ—ä¸­å“ªäº› token æœ‰æ•ˆï¼ŒåŒæ—¶é¿å…çœ‹åˆ°æœªæ¥çš„ä¿¡æ¯ã€‚

### æ•°æ®é›†

wmt16

### å®éªŒ

å‚æ•°

```python
dataset = WMT16Dataset(split="train", src_lang="de", tgt_lang="en", max_len=500, max_samples=5000, data_dir="./data")

model = make_model(len(dataset.src_vocab), len(dataset.tgt_vocab), N=2, d_model=128, d_ff=256, h=4)

train(model, dataset, epochs=30, batch_size=32, save_dir="model")
```

![image-20250913161248140](Transformer.assets/image-20250913161248140.png)

æ•ˆæœï¼šå·®

åŸå› ï¼šTransformeréœ€è¦è¶³å¤Ÿæ·±æ‰èƒ½å­¦ä¹ åˆ°è¶³å¤Ÿç‰¹å¾ï¼ŒåŒæ—¶d_modelä¹Ÿå¤ªå°

å‚æ•°

```python
dataset = WMT16Dataset(split="train", src_lang="de", tgt_lang="en", max_len=500, max_samples=10000, data_dir="./data")

model = make_model(len(dataset.src_vocab), len(dataset.tgt_vocab), N=2, d_model=512, d_ff=1024, h=4)

train(model, dataset, epochs=30, batch_size=32, save_dir="model")
```

![image-20250915152859143](Transformer.assets/image-20250915152859143.png)

æ•ˆæœï¼šå·®ï¼Œç¿»è¯‘è¯­ä¹‰å·®è·å¤§

åŸå› ï¼šæˆ‘è®¤ä¸ºæ˜¯modelä¸­å‚æ•°Nè¿‡å°ï¼Œå¯¼è‡´ä¸¤ä¸ªè¯­è¨€ä¸­è¯­ä¹‰äº¤æµç¼ºå¤±
