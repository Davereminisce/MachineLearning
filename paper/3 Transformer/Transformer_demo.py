import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
from Modular_Transformer import make_model
from tqdm import tqdm

# =======================
# 1. æ•°æ®é›†ç±»ï¼ˆwmt16 ç‰ˆæœ¬ï¼‰
# =======================
class WMT16Dataset(Dataset):
    def __init__(self, split="train", src_lang="de", tgt_lang="en", max_len=50, max_samples=500, data_dir="./data"):
        dataset = load_dataset("wmt16", "de-en", split=split, cache_dir=data_dir)
        if max_samples:
            dataset = dataset.select(range(max_samples))

        # âš ï¸ æ³¨æ„è¿™é‡Œè®¿é—® translation å­—æ®µ
        self.src_sentences = [ex['translation'][src_lang] for ex in dataset]
        self.tgt_sentences = [ex['translation'][tgt_lang] for ex in dataset]
        self.max_len = max_len

        # æ„å»ºè¯è¡¨
        # è¯è¡¨å¤§å° = æ‰€æœ‰è®­ç»ƒå¥å­ä¸­ä¸é‡å¤çš„ token æ•° + ç‰¹æ®Š tokenã€‚
        # åŸæœ¬çš„æ¯ä¸ªtokenå°±æ˜¯åœ¨è¯è¡¨ä¸­çš„ä¸€ä¸ªæ•°å­—ï¼Œé€šè¿‡embeddingåï¼Œå˜ä¸ºd_modelé•¿åº¦çš„å‘é‡äº†
        self.src_vocab = self.build_vocab(self.src_sentences)
        self.tgt_vocab = self.build_vocab(self.tgt_sentences)

    @staticmethod
    def build_vocab(sentences):
        vocab = {"<pad>":0, "<sos>":1, "<eos>":2}
        idx = 3
        for s in sentences:
            for w in s.strip().split():
                if w not in vocab:
                    vocab[w] = idx
                    idx += 1
        return vocab

    def encode_sentence(self, sentence, vocab):
        tokens = ["<sos>"] + sentence.strip().split() + ["<eos>"]
        ids = [vocab.get(t, 0) for t in tokens]
        if len(ids) < self.max_len:
            ids += [vocab["<pad>"]] * (self.max_len - len(ids))
        return ids[:self.max_len]

    def __len__(self):
        return len(self.src_sentences)

    def __getitem__(self, idx):
        src_ids = torch.tensor(self.encode_sentence(self.src_sentences[idx], self.src_vocab))
        tgt_ids = torch.tensor(self.encode_sentence(self.tgt_sentences[idx], self.tgt_vocab))
        return src_ids, tgt_ids

# =======================
# 2. Collate å‡½æ•°
# =======================
def collate_fn(batch):
    src_batch, tgt_batch = zip(*batch)
    src_batch = torch.stack(src_batch)
    tgt_batch = torch.stack(tgt_batch)
    return src_batch, tgt_batch

# =======================
# 3. Mask å‡½æ•°
# =======================
def create_masks(src, tgt, pad_idx):
    src_mask = (src != pad_idx).unsqueeze(1)
    tgt_len = tgt.size(1)
    tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=src.device)).bool()
    tgt_padding_mask = (tgt != pad_idx).unsqueeze(1)
    tgt_mask = tgt_mask & tgt_padding_mask
    return src_mask, tgt_mask

# =======================
# 4. è®­ç»ƒå‡½æ•°
# =======================
def train(model, dataset, epochs=5, lr=1e-3, batch_size=32, save_dir="model", device=None):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    os.makedirs(save_dir, exist_ok=True)

    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss(ignore_index=dataset.tgt_vocab["<pad>"])

    start_epoch = 0
    save_path = os.path.join(save_dir, "transformer_wmt16.pth")

    if os.path.exists(save_path):
        checkpoint = torch.load(save_path, map_location=device)
        model.load_state_dict(checkpoint["model_state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        start_epoch = checkpoint["epoch"] + 1
        print(f"ğŸ”„ Loaded model from epoch {checkpoint['epoch']+1}, loss={checkpoint['loss']:.4f}")

    for epoch in range(start_epoch, epochs):
        model.train()
        total_loss = 0
        # ä½¿ç”¨ tqdm åŒ…è£¹ DataLoaderï¼Œæ˜¾ç¤ºè¿›åº¦
        loop = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{epochs}")
        for src, tgt in loop:
            src, tgt = src.to(device), tgt.to(device)
            src_mask, tgt_mask = create_masks(src, tgt[:, :-1], dataset.src_vocab["<pad>"])

            optimizer.zero_grad()
            output = model(src, tgt[:, :-1], src_mask, tgt_mask)
            logits = model.generator(output)
            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1))
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

            # æ›´æ–°è¿›åº¦æ¡æè¿°æ˜¾ç¤ºå½“å‰ loss
            loop.set_postfix(loss=loss.item())

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1}/{epochs}, Loss = {avg_loss:.4f}")

        torch.save({
            "epoch": epoch,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "loss": avg_loss
        }, save_path)
        print(f"ğŸ’¾ Model saved at {save_path}")

# =======================
# 5. æ¨ç†å‡½æ•°
# =======================
def translate(model, sentence, dataset, max_len=50, device=None):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model.eval()
    with torch.no_grad():
        src_ids = torch.tensor(dataset.encode_sentence(sentence, dataset.src_vocab)).unsqueeze(0).to(device)
        src_mask = (src_ids != dataset.src_vocab["<pad>"]).unsqueeze(1).to(device)
        tgt_ids = torch.tensor([[dataset.tgt_vocab["<sos>"]]], device=device)

        for _ in range(max_len):
            tgt_mask = torch.tril(torch.ones(tgt_ids.size(1), tgt_ids.size(1), device=device)).bool().unsqueeze(0)
            out = model(src_ids, tgt_ids, src_mask, tgt_mask)
            logits = model.generator(out)
            next_word = logits[:, -1, :].argmax(dim=-1).unsqueeze(0)
            tgt_ids = torch.cat([tgt_ids, next_word], dim=1)
            if next_word.item() == dataset.tgt_vocab["<eos>"]:
                break

        id_to_word = {i: w for w, i in dataset.tgt_vocab.items()}
        words = [id_to_word[i.item()] for i in tgt_ids[0][1:]]
        if "<eos>" in words:
            words = words[:words.index("<eos>")]
        return " ".join(words)

# =======================
# 6. ä¸»ç¨‹åº
# =======================
if __name__ == "__main__":
    dataset = WMT16Dataset(split="train", src_lang="de", tgt_lang="en", max_len=500, max_samples=10000, data_dir="./data")
    '''
    å‚æ•°è¯´æ˜ï¼š
    split="train"     -> ä½¿ç”¨è®­ç»ƒé›†ï¼ˆtrainï¼‰ï¼Œå¯é€‰ "validation" æˆ– "test"    
        train       454885æ¡
        validation  2169æ¡
        test        2999æ¡
    src_lang="de"     -> æºè¯­è¨€ä¸ºå¾·è¯­ï¼Œæ¨¡å‹è¾“å…¥ä¸ºå¾·è¯­å¥å­
    tgt_lang="en"     -> ç›®æ ‡è¯­è¨€ä¸ºè‹±è¯­ï¼Œæ¨¡å‹è¾“å‡ºä¸ºè‹±è¯­å¥å­
    max_len=50        -> æ¯ä¸ªå¥å­æœ€å¤§é•¿åº¦ï¼Œè¶…è¿‡æˆªæ–­ï¼Œä¸è¶³è¡¥ <pad>
    max_samples=5000  -> æœ€å¤§å–æ ·æ•°é‡ä¸º 5000 æ¡ï¼Œç”¨äºå¿«é€Ÿè°ƒè¯•
    data_dir="./data" -> æ•°æ®ä¸‹è½½/ç¼“å­˜åˆ°é¡¹ç›®ç›®å½•ä¸‹çš„ ./data æ–‡ä»¶å¤¹
    '''

    model = make_model(len(dataset.src_vocab), len(dataset.tgt_vocab), N=2, d_model=512, d_ff=1024, h=4)
    '''
    å‚æ•°è¯´æ˜ï¼š
    len(dataset.src_vocab) -> æºè¯­è¨€è¯è¡¨å¤§å°ï¼ˆå¾·è¯­ï¼‰ï¼Œç”¨äºåµŒå…¥å±‚è¾“å…¥ç»´åº¦
    len(dataset.tgt_vocab) -> ç›®æ ‡è¯­è¨€è¯è¡¨å¤§å°ï¼ˆè‹±è¯­ï¼‰ï¼Œç”¨äºè¾“å‡ºå±‚ç”Ÿæˆæ¦‚ç‡
    N=2   -> Encoder å’Œ Decoder å †å çš„å±‚æ•°ï¼Œæ¯ä¸ªå †å åŒ…å«å¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œ
    d_model=128 -> Transformer ä¸­éšè—çŠ¶æ€å‘é‡ç»´åº¦ï¼Œä¹Ÿæ˜¯åµŒå…¥å‘é‡ç»´åº¦
    d_ff=256    -> å‰é¦ˆå…¨è¿æ¥å±‚çš„ç»´åº¦ï¼ˆFeed-Forward Networkï¼‰
    h=4        -> å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å¤´æ•°ï¼ˆhead æ•°ï¼‰ï¼Œæ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›
    '''

    train(model, dataset, epochs=30, batch_size=32, save_dir="model")


    while True:
        sentence = input("Enter German sentence (or 'quit' to exit): ")
        if sentence.lower() == "quit":
            break
        translation = translate(model, sentence, dataset)
        print("Translation:", translation)
    '''
    data = [
        ("i like cats", "ich mag katzen"),
        ("you love dogs", "du liebst hunde"),
        ("he eats food", "er isst essen"),
        ("we play games", "wir spielen spiele"),
        ("they read books", "sie lesen bÃ¼cher"),
    ]
    '''